{
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 68479,
          "databundleVersionId": 7609535,
          "sourceType": "competition"
        },
        {
          "sourceId": 7009925,
          "sourceType": "datasetVersion",
          "datasetId": 4030196
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 425.383835,
      "end_time": "2024-02-17T02:35:36.759686",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-02-17T02:28:31.375851",
      "version": "2.5.0"
    },
    "colab": {
      "name": "notebook87a4c88bd9",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roklp/MLP34/blob/main/notebook87a4c88bd9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style = \"font-size:300%; text-align:center;color:#0000FF; letter-spacing: 2px;padding: 10px;border-bottom: 5px solid #407A68\"> Obesity Risk Prediction (Multi-Class) </h1>"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "papermill": {
          "duration": 0.016816,
          "end_time": "2024-02-17T02:28:34.119183",
          "exception": false,
          "start_time": "2024-02-17T02:28:34.102367",
          "status": "completed"
        },
        "tags": [],
        "id": "U2SJKQKXk_lI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this kaggle project, We explored the task of predicting obesity risk using a multi-class classification approach.\n",
        "This report outlines the key steps taken in feature engineering, ensemble modeling, and encoding techniques to achieve accurate predictions.\n",
        "# feature engineering\n",
        "In this section, various transformations are applied to preprocess the data and derive new features that could potentially enhance the predictive performance of machine learning models.\n",
        "**Age and Height Rounding**  <br>\n",
        "To facilitate model learning, the 'Age' and 'Height' columns undergo rounding transformations. <br>The 'Age' values are multiplied by 100 and converted to uint16, effectively rounding them to the nearest whole number. Similarly, the 'Height' values are also multiplied by 100 and converted to uint16 to achieve rounding.\n",
        "**Feature Extraction** <br>\n",
        "New features are extracted to provide additional insights into the dataset. Specifically, the 'BMI' (Body Mass Index) feature is computed by dividing the weight by the square of the height. <br>This metric is commonly used to assess an individual's body composition. Additionally, a 'PseudoTarget' feature is created based on the BMI values. The BMI values are segmented into predefined bins, and each observation is assigned a categorical label corresponding to its BMI category.\n",
        "**Column Rounding** <br>\n",
        "Certain columns in the dataset have their values rounded to integers for simplification and standardization. This operation is performed on columns such as 'FCVC', 'NCP', 'CH2O', 'FAF', and 'TUE'. <br>By rounding these numerical values, the model can focus on broader patterns and trends within the data.\n",
        "**Feature Dropping** <br>\n",
        "A custom transformer, known as FeatureDropper, is employed to remove specified columns from the dataset. This allows for the elimination of redundant or irrelevant features that may hinder model performance. <br>The FeatureDropper class is initialized with a list of columns to be dropped, and during the transform process, these columns are excluded from the dataset.\n",
        "# encoding\n",
        "In this code, we primarily used OneHotEncoder and MEstimateEncoder.\n",
        "Looking at the train data, we have a mixture of numerical and categorical columns.\n",
        "Furthermore, the dependent variable we need to predict is categorical, not numerical.\n",
        "Therefore, we encoded the ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS'] columns in the train data,\n",
        "and the 'NObeyesdad' column in the test data.\n",
        "MEstimateEncoder is one method for encoding categorical variables. This method encodes each category of the categorical variable using the mean of the target variable for each category. Instead of using the mean of the target variable for each category, MEstimateEncoder calculates the mean of the category with a pre-specified M value. This allows for consistent calculation of the mean for all categories. This method is useful when the number of samples in a category is small or when categories are peculiar.\n",
        "# ensemble\n",
        "We employ an ensemble model consisting of Random Forest Model, LGBM Model, XGB Model, and CatBoost Model. For each model, we utilize Optuna to find the optimal hyperparameters, followed by cross-validation. Additionally, we assign weights to each model and create the final ensemble model. Finally, we compute the accuracy."
      ],
      "metadata": {
        "id": "AlFi6KkSmkJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List of libraries used in statistical analysis\n",
        "- matplotlib\n",
        "- numpy\n",
        "- pandas\n",
        "- scipy\n",
        "- seaborn\n",
        "#List of libraries used in the final model\n",
        "- os\n",
        "- tensorflow\n",
        "- random\n",
        "- warnings\n",
        "- numpy\n",
        "- pandas\n",
        "- matplotlib\n",
        "- seaborn\n",
        "- sklearn(make_pipeline, Pipeline, StandardScaler, MinMaxScaler, OneHotEncoder, CatBoostEncoder, MEstimateEncoder, StratifiedGroupKFold, RandomForestClassifier, RidgeClassifier, LogisticRegression, set_config, FunctionTransformer, StratifiedKFold, ColumnTransformer, make_column_transformer, clone, BaseEstimator, TransformerMixin, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, PCA, KMeans)\n",
        "- xgboost(XGBClassifier)\n",
        "- catboost(CatBoostClassifier)\n",
        "- lightgbm(LGBMClassifier)\n",
        "- optuna\n",
        "- prettytable(PrettyTable)"
      ],
      "metadata": {
        "id": "EDKS9MewmtPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The original work is available at https://www.kaggle.com/code/ksevta/ps4e2-xgb-lgbm-0-92**"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.015488,
          "end_time": "2024-02-17T02:28:34.150846",
          "exception": false,
          "start_time": "2024-02-17T02:28:34.135358",
          "status": "completed"
        },
        "tags": [],
        "id": "PRNogEjok_lJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: To learn from high scoring notebooks.\n",
        "\n",
        "Following are some modifications I have done in the notebook to improve score.\n",
        "\n",
        "- Random number fixed.\n",
        "- n_splits changed to 9."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.016436,
          "end_time": "2024-02-17T02:28:34.183045",
          "exception": false,
          "start_time": "2024-02-17T02:28:34.166609",
          "status": "completed"
        },
        "tags": [],
        "id": "VyUjufhJk_lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T07:13:34.998951Z",
          "iopub.execute_input": "2024-03-04T07:13:34.999289Z",
          "iopub.status.idle": "2024-03-04T07:13:51.485681Z",
          "shell.execute_reply.started": "2024-03-04T07:13:34.999254Z",
          "shell.execute_reply": "2024-03-04T07:13:51.48474Z"
        },
        "trusted": true,
        "id": "-5ut411Ck_lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf  # To ensure reproducibility of project results / Open-source platform for building and training machine learning and deep learning models\n",
        "import random as rn  # Provides various functions for generating random numbers\n",
        "os.listdir('/kaggle/input/playground-series-s4e2/')  # Used to check what files or folders are in the specified path\n",
        "os.environ['PYTHONHASHSEED'] = '51'  # Ensures consistent results of hash function / Increases result reproducibility\n",
        "rn.seed(89)  # Sets the seed number to 89 when running randomly / Ensures consistent results\n",
        "tf.random.set_seed(40)  # Sets the starting point of the random process to 40 / Used for dropout in tensorflow\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 13.369115,
          "end_time": "2024-02-17T02:28:47.567989",
          "exception": false,
          "start_time": "2024-02-17T02:28:34.198874",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T07:16:04.756847Z",
          "iopub.execute_input": "2024-03-04T07:16:04.757721Z",
          "iopub.status.idle": "2024-03-04T07:16:16.87562Z",
          "shell.execute_reply.started": "2024-03-04T07:16:04.757663Z",
          "shell.execute_reply": "2024-03-04T07:16:16.87482Z"
        },
        "trusted": true,
        "id": "_U3uJsGsk_lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "<div style=\"font-size:120%\">\n",
        "    <b>Goal:</b> We have to predict obesity risk in individuals.<br><br>\n",
        "    <b>Dataset Description:</b>\n",
        "</div>\n",
        "\n",
        "| Column | Full Form | Description|\n",
        "|---|---|---|\n",
        "| 'id'| id | Unique for each person(row)|\n",
        "|'Gender'| Gender| person's Gender|\n",
        "| 'Age' | Age| Dtype is float. Age is between 14 years to 61 years |\n",
        "|'Height'| Height | Height is in meter it's between 1.45m to 1.98m|\n",
        "| 'Weight' | Weight| Weight is between 39 to 165. I think it's in KG.|\n",
        "|'family_history_with_overweight'| family history <br> with overweight| yes or no question|\n",
        "| 'FAVC'| Frequent consumption <br> of high calorie food| it's yes or no question. i think question they asked is <br>do you consume high calorie food|\n",
        "|'FCVC'|  Frequency of <br>consumption of vegetables| Similar to FAVC. this is also `yes or no` question|\n",
        "|'NCP'| Number of main meals| dtype is float, NCP is between 1 & 4. I think it should be 1,2,3,4 <br>but our data is synthetic so it's taking float values|\n",
        "|'CAEC'| Consumption of <br>food between meals| takes 4 values `Sometimes`, `Frequently`, `no` & `Always` <br>|\n",
        "| 'SMOKE'| Smoke | yes or no question. i think the question is \"Do you smoke?\" |\n",
        "|'CH2O'| Consumption of <br>water daily| CH2O takes values between 1 & 3. again it's given as <br>float may be because of synthetic data. it's values should be 1,2 or 3|\n",
        "|'SCC'|  Calories consumption <br>monitoring| yes or no question|\n",
        "|'FAF'| Physical activity <br>frequency| FAF is between 0 to 3, 0 means no physical activity<br> and 3 means high workout. and again, in our data it's given as float|\n",
        "|'TUE'| Time using <br>technology devices| TUE is between 0 to 2. I think question will be \"How long you have <br>been using technology devices to track your health.\" in our data it's given as float |\n",
        "|'CALC'| Consumption of alcohol | Takes 3 values: `Sometimes`, `no`, `Frequently`|\n",
        "| 'MTRANS' | Transportation used| MTRANS takes 5 values `Public_Transportation`, `Automobile`, <br>`Walking`, `Motorbike`, & `Bike`|\n",
        "|'NObeyesdad'| TARGET | This is our target, takes 7 values, and in this comp. we have to give <br>the class name (Not the Probability, which is the case in most comp.)\n",
        "\n",
        "\n",
        "<div style=\"font-size:120%\">\n",
        "    <b>NObeyesdad (Target Variable):</b>\n",
        "</div>\n",
        "\n",
        "* Insufficient_Weight : Less than 18.5\n",
        "* Normal_Weight       : 18.5 to 24.9\n",
        "* Obesity_Type_I      : 30.0 to 34.9\n",
        "* Obesity_Type_II     : 35.0 to 39.9\n",
        "* Obesity_Type_III   : Higher than 40\n",
        "* Overweight_Level_I, Overweight_Level_II takes values between 25 to 29\n",
        "\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.015339,
          "end_time": "2024-02-17T02:28:47.599268",
          "exception": false,
          "start_time": "2024-02-17T02:28:47.583929",
          "status": "completed"
        },
        "tags": [],
        "id": "-ec04TJAk_lK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.015755,
          "end_time": "2024-02-17T02:28:47.630572",
          "exception": false,
          "start_time": "2024-02-17T02:28:47.614817",
          "status": "completed"
        },
        "tags": [],
        "id": "NC1PNcrrk_lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from category_encoders import OneHotEncoder, CatBoostEncoder, MEstimateEncoder\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
        "\n",
        "from sklearn import set_config\n",
        "import os\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import optuna\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.base import clone\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "import optuna\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "papermill": {
          "duration": 6.21601,
          "end_time": "2024-02-17T02:28:53.861903",
          "exception": false,
          "start_time": "2024-02-17T02:28:47.645893",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T08:03:38.090703Z",
          "iopub.execute_input": "2024-03-04T08:03:38.091295Z",
          "iopub.status.idle": "2024-03-04T08:03:38.100124Z",
          "shell.execute_reply.started": "2024-03-04T08:03:38.091264Z",
          "shell.execute_reply": "2024-03-04T08:03:38.099023Z"
        },
        "trusted": true,
        "id": "5i5DOaljk_lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.01553,
          "end_time": "2024-02-17T02:28:53.893726",
          "exception": false,
          "start_time": "2024-02-17T02:28:53.878196",
          "status": "completed"
        },
        "tags": [],
        "id": "vGgoNPX9k_lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Parameters for Reproducibility\n",
        "pd.set_option(\"display.max_rows\", 100)\n",
        "FILE_PATH = \"/kaggle/input/playground-series-s4e2/\"\n",
        "TARGET = \"NObeyesdad\"\n",
        "n_splits = 9  # Used to evaluate the performance of the model by dividing the data into multiple parts and using each part for training and validation sequentially\n",
        "RANDOM_SEED = 73  # Set the random seed to 73, ensuring reproducibility by obtaining the same result every time random operations are performed\n"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "papermill": {
          "duration": 0.024371,
          "end_time": "2024-02-17T02:28:53.933664",
          "exception": false,
          "start_time": "2024-02-17T02:28:53.909293",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T08:04:04.872769Z",
          "iopub.execute_input": "2024-03-04T08:04:04.873148Z",
          "iopub.status.idle": "2024-03-04T08:04:04.878208Z",
          "shell.execute_reply.started": "2024-03-04T08:04:04.873117Z",
          "shell.execute_reply": "2024-03-04T08:04:04.87726Z"
        },
        "trusted": true,
        "id": "IXmi99yik_lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.015128,
          "end_time": "2024-02-17T02:28:53.965535",
          "exception": false,
          "start_time": "2024-02-17T02:28:53.950407",
          "status": "completed"
        },
        "tags": [],
        "id": "zI5BHkOUk_lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load all data\n",
        "train = pd.read_csv(os.path.join(FILE_PATH, \"train.csv\"))\n",
        "test = pd.read_csv(os.path.join(FILE_PATH, \"test.csv\"))\n",
        "sample_sub = pd.read_csv(os.path.join(FILE_PATH, \"sample_submission.csv\"))\n",
        "train_org = pd.read_csv(\"/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv\")"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.225115,
          "end_time": "2024-02-17T02:28:54.206038",
          "exception": false,
          "start_time": "2024-02-17T02:28:53.980923",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T08:04:06.812557Z",
          "iopub.execute_input": "2024-03-04T08:04:06.812916Z",
          "iopub.status.idle": "2024-03-04T08:04:06.953652Z",
          "shell.execute_reply.started": "2024-03-04T08:04:06.812885Z",
          "shell.execute_reply": "2024-03-04T08:04:06.952802Z"
        },
        "trusted": true,
        "id": "zkqE08Oxk_lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore Data"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.015537,
          "end_time": "2024-02-17T02:28:54.238407",
          "exception": false,
          "start_time": "2024-02-17T02:28:54.22287",
          "status": "completed"
        },
        "tags": [],
        "id": "3yJc57fwk_lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prettify_df(df):\n",
        "    table = PrettyTable()\n",
        "    table.field_names = df.columns\n",
        "\n",
        "    for row in df.values:\n",
        "        table.add_row(row)\n",
        "    print(table)\n",
        "# Define a function to prettify the DataFrame (tabular data)\n",
        "# Use the PrettyTable library to display the contents of the DataFrame in a visually appealing tabular format\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.023281,
          "end_time": "2024-02-17T02:28:54.277296",
          "exception": false,
          "start_time": "2024-02-17T02:28:54.254015",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T08:04:09.700144Z",
          "iopub.execute_input": "2024-03-04T08:04:09.700527Z",
          "iopub.status.idle": "2024-03-04T08:04:09.706157Z",
          "shell.execute_reply.started": "2024-03-04T08:04:09.700499Z",
          "shell.execute_reply": "2024-03-04T08:04:09.705144Z"
        },
        "trusted": true,
        "id": "_f6eb-XCk_lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.04907,
          "end_time": "2024-02-17T02:28:54.342091",
          "exception": false,
          "start_time": "2024-02-17T02:28:54.293021",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T07:18:12.652735Z",
          "iopub.execute_input": "2024-03-04T07:18:12.653429Z",
          "iopub.status.idle": "2024-03-04T07:18:12.683832Z",
          "shell.execute_reply.started": "2024-03-04T07:18:12.6534Z",
          "shell.execute_reply": "2024-03-04T07:18:12.682855Z"
        },
        "trusted": true,
        "id": "GDFEIT3Bk_lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Data\n",
        "print(\"Train Data\")\n",
        "print(f\"Total number of rows: {len(train)}\")\n",
        "print(f\"Total number of columns: {train.shape[1]}\\n\")\n",
        "\n",
        "# Test Data\n",
        "print(\"Test Data\")\n",
        "print(f\"Total number of rows: {len(test)}\")\n",
        "print(f\"Total number of columns: {test.shape[1]}\")\n",
        "# Print the number of rows and columns in the Train and Test Data\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.026179,
          "end_time": "2024-02-17T02:28:54.386933",
          "exception": false,
          "start_time": "2024-02-17T02:28:54.360754",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T08:04:11.676308Z",
          "iopub.execute_input": "2024-03-04T08:04:11.677305Z",
          "iopub.status.idle": "2024-03-04T08:04:11.683671Z",
          "shell.execute_reply.started": "2024-03-04T08:04:11.677268Z",
          "shell.execute_reply": "2024-03-04T08:04:11.682578Z"
        },
        "trusted": true,
        "id": "yRGbbEmwk_lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check null and unique count\n",
        "# FHWO: family_history_with_overweight\n",
        "train_copy = train.rename(columns={\"family_history_with_overweight\":\"FHWO\"}) # Renaming column 'family_history_with_overweight' to 'FHWO'\n",
        "tmp = pd.DataFrame(index=train_copy.columns) # Creating a new DataFrame for columns\n",
        "tmp['count'] = train_copy.count() # Calculating the non-null count\n",
        "tmp['dtype'] = train_copy.dtypes # Finding the data types of columns\n",
        "tmp['nunique'] = train_copy.nunique() # Calculating the number of unique values for each column\n",
        "tmp['%nunique'] = (tmp['nunique']/len(train_copy))*100 # Calculating the percentage of unique values for each column relative to the total number of rows\n",
        "tmp['%null'] = (train_copy.isnull().sum()/len(train_copy))*100 # Calculating the percentage of null values for each column\n",
        "tmp['min'] = train_copy.min() # Finding the minimum value for each column\n",
        "tmp['max'] = train_copy.max() # Finding the maximum value for each column\n",
        "tmp\n",
        "\n",
        "tmp.reset_index(inplace=True) # Resetting the index of the resulting DataFrame\n",
        "tmp = tmp.rename(columns = {\"index\":\"Column Name\"}) # Renaming the 'index' column to 'Column Name'\n",
        "tmp = tmp.round(3) # Rounding the values in the DataFrame to 3 decimal places\n",
        "prettify_df(tmp) # Printing the prettified DataFrame\n",
        "del tmp, train_copy # Deleting temporary variables to free up memory\n",
        "# Explains the process of examining important information about each column (feature) in the training dataset and pretty printing the results.\n",
        "# Specifically, it calculates and displays the percentage of null values, number and percentage of unique values, data type, minimum, and maximum values for each column.\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "papermill": {
          "duration": 0.182219,
          "end_time": "2024-02-17T02:28:54.587302",
          "exception": false,
          "start_time": "2024-02-17T02:28:54.405083",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T08:04:14.659173Z",
          "iopub.execute_input": "2024-03-04T08:04:14.659912Z",
          "iopub.status.idle": "2024-03-04T08:04:14.802628Z",
          "shell.execute_reply.started": "2024-03-04T08:04:14.659876Z",
          "shell.execute_reply": "2024-03-04T08:04:14.80134Z"
        },
        "trusted": true,
        "id": "qi3JU2zhk_lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target Distribution with Gender\n",
        "# Analyzing the number and proportion of data points by gender\n",
        "# Checking the proportion of each target class in the entire dataset\n",
        "pd.set_option('display.float_format', '{:.2f}'.format) # Setting Pandas to display numbers up to two decimal places\n",
        "tmp = pd.DataFrame(train.groupby([TARGET,'Gender'])[\"id\"].agg('count')) # Grouping by variables with the count of the 'id' column\n",
        "tmp.columns = ['Count']\n",
        "train[TARGET].value_counts() # Calculating the overall distribution of values for the TARGET variable\n",
        "tmp = pd.merge(tmp,train[TARGET].value_counts(),left_index=True, right_index=True)\n",
        "tmp.columns = ['gender_count','target_class_count']\n",
        "tmp['%gender_count'] = tmp['gender_count']/tmp['target_class_count']\n",
        "tmp[\"%target_class_count\"] = tmp['target_class_count']/len(train)\n",
        "tmp = tmp[['gender_count','%gender_count','target_class_count','%target_class_count']]\n",
        "print(\"Target Distribution with Gender\")\n",
        "tmp\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "papermill": {
          "duration": 0.069893,
          "end_time": "2024-02-17T02:28:54.676906",
          "exception": false,
          "start_time": "2024-02-17T02:28:54.607013",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T08:04:16.628178Z",
          "iopub.execute_input": "2024-03-04T08:04:16.628568Z",
          "iopub.status.idle": "2024-03-04T08:04:16.662899Z",
          "shell.execute_reply.started": "2024-03-04T08:04:16.628539Z",
          "shell.execute_reply": "2024-03-04T08:04:16.661993Z"
        },
        "trusted": true,
        "id": "oZOX_9Tkk_lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_num_cols = list(train.select_dtypes(\"float\").columns)\n",
        "raw_cat_cols = list(train.columns.drop(raw_num_cols+[TARGET]))\n",
        "\n",
        "full_form = dict({'FAVC' : \"Frequent consumption of high caloric food\",\n",
        "                  'FCVC' : \"Frequency of consumption of vegetables\",\n",
        "                  'NCP' :\"Number of main meal\",\n",
        "                  'CAEC': \"Consumption of food between meals\",\n",
        "                  'CH2O': \"Consumption of water daily\",\n",
        "                  'SCC':  \"Calories consumption monitoring\",\n",
        "                  'FAF': \"Physical activity frequency\",\n",
        "                  'TUE': \"Time using technology devices\",\n",
        "                  'CALC': \"Consumption of alcohol\" ,\n",
        "                  'MTRANS' : \"Transportation used\"})\n",
        "\n",
        "# Dividing the columns of the dataset into numerical and categorical, and providing full descriptions for some column abbreviations\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "papermill": {
          "duration": 0.029042,
          "end_time": "2024-02-17T02:28:54.72462",
          "exception": false,
          "start_time": "2024-02-17T02:28:54.695578",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T08:04:18.654332Z",
          "iopub.execute_input": "2024-03-04T08:04:18.654708Z",
          "iopub.status.idle": "2024-03-04T08:04:18.66248Z",
          "shell.execute_reply.started": "2024-03-04T08:04:18.654679Z",
          "shell.execute_reply": "2024-03-04T08:04:18.661522Z"
        },
        "trusted": true,
        "id": "rn1wI19Yk_lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From Above Table, We Can See**\n",
        "* All the People in `Obesity_Type_II` are **Male** and in `Obesity_Type_III` all are **Female**\n",
        "* `Overweight_Level_II` consists `70%` **Male**, and `Insufficient_Weight` consists more than `60%` **Female**\n",
        "* From these point we can say that `Gender` is a important feature for the Obesity Prediction\n",
        "\n",
        "* All individuals classified under Obesity_Type_II are male, while those in Obesity_Type_III are all female.\n",
        "\n",
        "* About 70% of individuals in Overweight_Level_II are male, and more than 60% of those in Insufficient_Weight are female.\n",
        "\n",
        "* Based on these observations, it can be inferred that Gender is an important feature for predicting obesity.\n",
        "\n",
        "* Translation seems good, but there's a small typo in the last sentence: \"...it can be inferred that Gender is an important feature for predicting obesity.\" I just corrected the typo."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017192,
          "end_time": "2024-02-17T02:28:54.760375",
          "exception": false,
          "start_time": "2024-02-17T02:28:54.743183",
          "status": "completed"
        },
        "tags": [],
        "id": "aVC7OZDRk_lM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization\n",
        "In this Section we will see:\n",
        "* Individual Numerical Plots\n",
        "* Individual Categorical Plots\n",
        "* Numerical Correlation Plot\n",
        "* Combined Numerical Plots"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017251,
          "end_time": "2024-02-17T02:28:54.794846",
          "exception": false,
          "start_time": "2024-02-17T02:28:54.777595",
          "status": "completed"
        },
        "tags": [],
        "id": "mIel-F4hk_lN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target Distribution with Gender\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.019064,
          "end_time": "2024-02-17T02:28:54.831257",
          "exception": false,
          "start_time": "2024-02-17T02:28:54.812193",
          "status": "completed"
        },
        "tags": [],
        "id": "9UGUnbOek_lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize = (12,5))\n",
        "plt.suptitle(\"Target Distribution\")\n",
        "\n",
        "sns.histplot(binwidth=0.5, x=TARGET, data=train, hue='Gender', palette=\"Blues\", ax=axs[0], discrete=True)\n",
        "\n",
        "axs[0].tick_params(axis='x', rotation=60)\n",
        "\n",
        "axs[1].pie(\n",
        "    train[TARGET].value_counts(),\n",
        "    shadow=True,\n",
        "    explode=[.1 for i in range(train[TARGET].nunique())],\n",
        "    labels=train[TARGET].value_counts().index,\n",
        "    autopct='%1.f%%',\n",
        "    colors=sns.color_palette(\"Blues\", n_colors=len(train[TARGET].value_counts()))\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Generate two graphs to display the distribution of the target variable (with respect to gender): a histogram and a pie chart."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.613817,
          "end_time": "2024-02-17T02:28:55.505391",
          "exception": false,
          "start_time": "2024-02-17T02:28:54.891574",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T07:43:43.805206Z",
          "iopub.execute_input": "2024-03-04T07:43:43.805577Z",
          "iopub.status.idle": "2024-03-04T07:43:44.504483Z",
          "shell.execute_reply.started": "2024-03-04T07:43:43.805549Z",
          "shell.execute_reply": "2024-03-04T07:43:44.503547Z"
        },
        "trusted": true,
        "id": "ZQxgJEJmk_lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"section_1\"> </a>\n",
        "# Individual Numerical Plots"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-06T08:06:50.621405Z",
          "iopub.status.busy": "2024-02-06T08:06:50.620976Z",
          "iopub.status.idle": "2024-02-06T08:06:50.629708Z",
          "shell.execute_reply": "2024-02-06T08:06:50.627713Z",
          "shell.execute_reply.started": "2024-02-06T08:06:50.621376Z"
        },
        "papermill": {
          "duration": 0.018586,
          "end_time": "2024-02-17T02:28:55.542561",
          "exception": false,
          "start_time": "2024-02-17T02:28:55.523975",
          "status": "completed"
        },
        "tags": [],
        "id": "1jlOi6mdk_lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axs = plt.subplots(len(raw_num_cols),1,figsize=(12,len(raw_num_cols)*2.5),sharex=False)\n",
        "for i, col in enumerate(raw_num_cols):\n",
        "    sns.violinplot(x=TARGET, y=col,hue=\"Gender\", data=train,ax = axs[i], split=False)\n",
        "    if col in full_form.keys():\n",
        "        axs[i].set_ylabel(full_form[col])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "#Create violin plots for each numerical variable (raw_num_cols) based on the target variable (TARGET) and gender (Gender\n",
        "# Each plot represents the distribution of the variable split by gender.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 5.540023,
          "end_time": "2024-02-17T02:29:01.101297",
          "exception": false,
          "start_time": "2024-02-17T02:28:55.561274",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T07:44:41.604001Z",
          "iopub.execute_input": "2024-03-04T07:44:41.605026Z",
          "iopub.status.idle": "2024-03-04T07:44:47.011442Z",
          "shell.execute_reply.started": "2024-03-04T07:44:41.604993Z",
          "shell.execute_reply": "2024-03-04T07:44:47.010462Z"
        },
        "trusted": true,
        "id": "9pVitls0k_lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights from above plots:\n",
        "* We should Ignore, **Female** distribution in `Obesity_Type_II` Class & **Male** distribution in \"Obesity_Type_III\". because of very small sample size\n",
        "* We can see People in category of `Insufficient Weight` consumes higher `Number of main Meal` maybe because to gain weight\n",
        "* `Frequency of consumption of Vegetables` is **Three** for everyone in class `Obesity Type III`\n",
        "* `Weight`, `Height` & `Gender` looks like the most important features. `Weight` shows very clear differentiation for diff classes\n",
        "\n",
        "\n",
        "* Due to very small sample sizes, we should disregard the distribution of females in the 'Obesity Type II' class and males in the 'Obesity Type III' class.\n",
        "* Individuals in the 'Insufficient Weight' category seem to consume more main meals.\n",
        "* All individuals in the 'Obesity Type III' class have a frequency of consuming vegetables three times.\n",
        "* Weight, Height, and Gender appear to be the most important features. Weight shows significant differentiation across various classes."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.025065,
          "end_time": "2024-02-17T02:29:01.151292",
          "exception": false,
          "start_time": "2024-02-17T02:29:01.126227",
          "status": "completed"
        },
        "tags": [],
        "id": "hDPUKkyqk_lN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"section_2\"> </a>\n",
        "# Individual Categorical Plots"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.025011,
          "end_time": "2024-02-17T02:29:01.200769",
          "exception": false,
          "start_time": "2024-02-17T02:29:01.175758",
          "status": "completed"
        },
        "tags": [],
        "id": "wxcRpMzdk_lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_,axs = plt.subplots(int(len(raw_cat_cols)-1),2,figsize=(12,len(raw_cat_cols)*3),width_ratios=[1, 4])\n",
        "for i,col in enumerate(raw_cat_cols[1:]):\n",
        "    sns.countplot(y=col,data=train,palette=\"bright\",ax=axs[i,0])\n",
        "    sns.countplot(x=col,data=train,hue=TARGET,palette=\"bright\",ax=axs[i,1])\n",
        "    if col in full_form.keys():\n",
        "        axs[i,0].set_ylabel(full_form[col])\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Generate two types of count plots for each categorical variable\n",
        "# The plot on the left shows the distribution of the variable\n",
        "# The plot on the right shows the distribution of the variable based on the target variable (TARGET)\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 3.816499,
          "end_time": "2024-02-17T02:29:05.043665",
          "exception": false,
          "start_time": "2024-02-17T02:29:01.227166",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T07:46:35.267926Z",
          "iopub.execute_input": "2024-03-04T07:46:35.268858Z",
          "iopub.status.idle": "2024-03-04T07:46:38.969052Z",
          "shell.execute_reply.started": "2024-03-04T07:46:35.268824Z",
          "shell.execute_reply": "2024-03-04T07:46:38.968137Z"
        },
        "trusted": true,
        "id": "R9un-Uk1k_lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"section_3\"></a>\n",
        "# Numerical Correlation Plot"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.029625,
          "end_time": "2024-02-17T02:29:05.10318",
          "exception": false,
          "start_time": "2024-02-17T02:29:05.073555",
          "status": "completed"
        },
        "tags": [],
        "id": "a7P-gpqsk_lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = train[raw_num_cols].corr(\"pearson\")\n",
        "sns.heatmap(tmp,annot=True,cmap =\"Blues\")\n",
        "# Compute the Pearson correlation coefficient among numerical variables (raw_num_cols) in the train dataset and visualize the result as a heatmap\n",
        "# Pearson correlation coefficient:\n",
        "# - Helps to understand the relationship between variables\n",
        "# - Assists in selecting important variables for machine learning modeling\n",
        "# - Enables early detection and resolution of multicollinearity issues in machine learning\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.848359,
          "end_time": "2024-02-17T02:29:05.980603",
          "exception": false,
          "start_time": "2024-02-17T02:29:05.132244",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T07:58:45.944532Z",
          "iopub.execute_input": "2024-03-04T07:58:45.944892Z",
          "iopub.status.idle": "2024-03-04T07:58:46.468702Z",
          "shell.execute_reply.started": "2024-03-04T07:58:45.944863Z",
          "shell.execute_reply": "2024-03-04T07:58:46.46782Z"
        },
        "trusted": true,
        "id": "yeBdkhH4k_lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `Height` has a positive corr with `Weight`,`FAF`. we will see there combined plots\n",
        "* People with higher `Weight` drinks more water.\n",
        "\n",
        "* Height shows a positive correlation with Weight and FAF (Physical activity frequency).\n",
        "* We will examine their combined plots.\n",
        "People with higher weight tend to consume more water."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.031747,
          "end_time": "2024-02-17T02:29:06.053088",
          "exception": false,
          "start_time": "2024-02-17T02:29:06.021341",
          "status": "completed"
        },
        "tags": [],
        "id": "wfV7kLQXk_lO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"section_4\"></a>\n",
        "# Combined Numerical Plots"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.031132,
          "end_time": "2024-02-17T02:29:06.115074",
          "exception": false,
          "start_time": "2024-02-17T02:29:06.083942",
          "status": "completed"
        },
        "tags": [],
        "id": "cS52Ii1Lk_lO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.jointplot(data=train, x=\"Height\", y=\"Weight\", hue=TARGET, height=6, palette=\"Blues\")\n",
        "\n",
        "#Creating a joint plot to visualize the relationship between \"Height\" and \"Weight\" using the train dataset.\n",
        "#The TARGET variable is used to distinguish the color of each data point."
      ],
      "metadata": {
        "papermill": {
          "duration": 3.525322,
          "end_time": "2024-02-17T02:29:09.675943",
          "exception": false,
          "start_time": "2024-02-17T02:29:06.150621",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T07:59:15.184937Z",
          "iopub.execute_input": "2024-03-04T07:59:15.185705Z",
          "iopub.status.idle": "2024-03-04T07:59:18.77142Z",
          "shell.execute_reply.started": "2024-03-04T07:59:15.18567Z",
          "shell.execute_reply": "2024-03-04T07:59:18.770638Z"
        },
        "trusted": true,
        "id": "X-EteXb6k_lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.jointplot(data=train, x=\"Age\", y=\"Height\", hue=TARGET,height=6, palette=\"Blues\")\n",
        "#Creating a joint plot to visualize the relationship between \"Height\" and \"Age\" using the train dataset.\n",
        "#The TARGET variable is used to distinguish the color of each data point."
      ],
      "metadata": {
        "papermill": {
          "duration": 3.477762,
          "end_time": "2024-02-17T02:29:13.186876",
          "exception": false,
          "start_time": "2024-02-17T02:29:09.709114",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T07:59:44.910022Z",
          "iopub.execute_input": "2024-03-04T07:59:44.910996Z",
          "iopub.status.idle": "2024-03-04T07:59:48.206665Z",
          "shell.execute_reply.started": "2024-03-04T07:59:44.91096Z",
          "shell.execute_reply": "2024-03-04T07:59:48.205718Z"
        },
        "trusted": true,
        "id": "-NOKl5Ack_lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA) & KMeans\n",
        "These plots are inspired by [This](https://www.kaggle.com/competitions/playground-series-s4e2/discussion/472471) discussion."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.035868,
          "end_time": "2024-02-17T02:29:13.25866",
          "exception": false,
          "start_time": "2024-02-17T02:29:13.222792",
          "status": "completed"
        },
        "tags": [],
        "id": "757kYSl-k_lO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# PCA\n",
        "# Enhances computational efficiency and reduces the risk of overfitting.\n",
        "# Allows reduction of data complexity while retaining important information.\n",
        "# Visualization: Makes it easier to understand patterns or structures in the data and facilitates clearer interpretation of relationships between variables.\n",
        "pca = PCA(n_components=2)  # Reduce to 2 principal components\n",
        "pca_top_2 = pca.fit_transform(train[raw_num_cols])\n",
        "# Apply PCA to the train dataset containing only numerical variables to transform each data point into a new form represented by 2 principal components\n",
        "\n",
        "tmp = pd.DataFrame(data=pca_top_2, columns=['pca_1', 'pca_2'])\n",
        "# Results are stored in pca_top_2, which is then used to create the tmp DataFrame\n",
        "tmp['TARGET'] = train[TARGET]\n",
        "# This DataFrame includes the transformed principal component values (pca_1, pca_2) and the target variable (TARGET)\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, figsize=(12, 6))\n",
        "sns.scatterplot(data=tmp, y=\"pca_1\", x=\"pca_2\", hue='TARGET', ax=axs[0])\n",
        "axs[0].set_title(\"Top 2 Principal Components\")\n",
        "# Set the title \"Top 2 Principal Components\" for the first subplot\n",
        "\n",
        "# KMeans\n",
        "# Reveals characteristics of each group.\n",
        "# Enables identification of hidden insights in the data.\n",
        "# By applying K-means clustering to data dimensionally reduced by PCA, the performance of the clustering algorithm can be further improved.\n",
        "kmeans = KMeans(7, random_state=RANDOM_SEED)  # Create a K-means clustering model with 7 clusters\n",
        "kmeans.fit(tmp[['pca_1', 'pca_2']])  # Apply K-means clustering to the PCA-transformed data\n",
        "sns.scatterplot(y=tmp['pca_1'], x=tmp['pca_2'], c=kmeans.labels_, cmap='viridis', marker='o', edgecolor='k', s=50, alpha=0.8, ax=axs[1])\n",
        "# Plot the clustering results in the second subplot (axs[1]) as a scatter plot\n",
        "axs[1].set_title(\"Kmean Clustering on First 2 Principal Components\")\n",
        "plt.\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 4.241619,
          "end_time": "2024-02-17T02:29:17.535748",
          "exception": false,
          "start_time": "2024-02-17T02:29:13.294129",
          "status": "completed"
        },
        "tags": [],
        "id": "hru8rMlqk_lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering & Processing"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.042339,
          "end_time": "2024-02-17T02:29:17.622105",
          "exception": false,
          "start_time": "2024-02-17T02:29:17.579766",
          "status": "completed"
        },
        "tags": [],
        "id": "1-72nZzpk_lO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In the age_rounder and height_rounder functions, values are multiplied by some value, which sometimes improves the model's CV score.\n",
        "# In the extract_features function, we combine features to generate new features, transforming the data to potentially improve the model's performance.\n",
        "\n",
        "def age_rounder(x):\n",
        "    x_copy = x.copy()\n",
        "    x_copy['Age'] = (x_copy['Age'] * 100).astype(np.uint16)  # Multiply the values in the 'Age' column by 100 and convert to integers to adjust scaling for better modeling.\n",
        "    return x_copy\n",
        "\n",
        "def height_rounder(x):\n",
        "    x_copy = x.copy()\n",
        "    x_copy['Height'] = (x_copy['Height'] * 100).astype(np.uint16)  # Multiply the values in the 'Height' column by 100 and convert to integers.\n",
        "    return x_copy\n",
        "\n",
        "def extract_features(x):\n",
        "    x_copy = x.copy()\n",
        "    x_copy['BMI'] = (x_copy['Weight'] / x_copy['Height'] ** 2)  # Create a new feature 'BMI'. This new feature helps the model better understand the data.\n",
        "    x_copy['PseudoTarget'] = pd.cut(x_copy['BMI'], bins=[0, 18.4, 24.9, 29, 34.9, 39.9, 100], labels=[0, 1, 2, 3, 4, 5])\n",
        "    return x_copy\n",
        "\n",
        "def col_rounder(x):  # Round the values in specific columns ('FCVC', 'NCP', 'CH2O', 'FAF', 'TUE') and convert them to integers to simplify for the model.\n",
        "    x_copy = x.copy()\n",
        "    cols_to_round = ['FCVC', \"NCP\", \"CH2O\", \"FAF\", \"TUE\"]\n",
        "    for col in cols_to_round:\n",
        "        x_copy[col] = round(x_copy[col])\n",
        "        x_copy[col] = x_copy[col].astype('int')\n",
        "    return x_copy\n",
        "\n",
        "# Each function operates on a copy of the original DataFrame and returns the transformed DataFrame.\n",
        "# These preprocessing steps clarify the features of the data and reduce unnecessary variability to potentially enhance model performance.\n",
        "\n",
        "AgeRounder = FunctionTransformer(age_rounder)\n",
        "HeightRounder = FunctionTransformer(height_rounder)\n",
        "ExtractFeatures = FunctionTransformer(extract_features)\n",
        "ColumnRounder = FunctionTransformer(col_rounder)\n",
        "# Use FunctionTransformer to make each preprocessing function compatible with Scikit-learn pipelines.\n",
        "# This allows for more efficient management of data preprocessing steps and easy integration into the model training pipeline.\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.052948,
          "end_time": "2024-02-17T02:29:17.715633",
          "exception": false,
          "start_time": "2024-02-17T02:29:17.662685",
          "status": "completed"
        },
        "tags": [],
        "id": "InQACaNXk_lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The FeatureDropper class, which inherits from Scikit-learn's transformers, is defined to remove specific columns from a DataFrame.\n",
        "# This is important when passing different sets of features to different models.\n",
        "# It provides flexibility to remove unnecessary features during data preprocessing, potentially positively impacting model performance.\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class FeatureDropper(BaseEstimator, TransformerMixin):\n",
        "    # FeatureDropper is a tool for removing unwanted columns (i.e., features or variables) from the data.\n",
        "    # It follows the conventions of the Scikit-learn library provided in Python.\n",
        "\n",
        "    def __init__(self, cols):  # In this part, FeatureDropper decides which columns to remove.\n",
        "        self.cols = cols  # When instantiated, it remembers the names of columns to remove.\n",
        "\n",
        "    def fit(self, x, y):  # Here, it defines a convention ensuring that all Scikit-learn tools work similarly.\n",
        "        return self\n",
        "\n",
        "    def transform(self, x):  # It actually removes the unwanted columns from the data.\n",
        "        return x.drop(self.cols, axis=1)\n",
        "\n",
        "# FeatureDropper is used to remove specific columns from the data when desired.\n",
        "# It facilitates easy experimentation with different data for different models.\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.050663,
          "end_time": "2024-02-17T02:29:17.807532",
          "exception": false,
          "start_time": "2024-02-17T02:29:17.756869",
          "status": "completed"
        },
        "tags": [],
        "id": "X_fqudQDk_lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next we will define `cross_val_model` which will be used to train and validate all the models we will use in this Notebook\n",
        "`cross_val_model` function gives three things: **val_scores**, **valid_predictions**, **test_predictions**\n",
        "* <b>val_scores:</b> This gives us accuracy score on Validation Data.\n",
        "* <b>valid_predictions:</b> This is a array which stores model predictions on validation set\n",
        "* <b>test_predictions:</b> This gives test prediction averaged by number of splits we are using\n",
        "\n",
        "### Next, we'll define `cross_val_model`, which will be used to train and validate all models in this notebook. The function provides three outputs: **val_scores**.\n",
        " **valid_predictions**, **test_predictions**\n",
        "* <b>val_scores:</b> Provides accuracy scores for the validation data.\n",
        "* <b>valid_predictions:</b> Stores model predictions for the validation set in an array.\n",
        "* <b>test_predictions:</b> Provides averaged test predictions based on the number of splits used."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.041714,
          "end_time": "2024-02-17T02:29:17.890213",
          "exception": false,
          "start_time": "2024-02-17T02:29:17.848499",
          "status": "completed"
        },
        "tags": [],
        "id": "4xABnV2ak_lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In cross_val_model we cross-validate models using Stratified K-Fold.\n",
        "\n",
        "# Encoding target values with integers\n",
        "target_mapping = {\n",
        "                  'Insufficient_Weight': 0,\n",
        "                  'Normal_Weight': 1,\n",
        "                  'Overweight_Level_I': 2,\n",
        "                  'Overweight_Level_II': 3,\n",
        "                  'Obesity_Type_I': 4,\n",
        "                  'Obesity_Type_II': 5 ,\n",
        "                  'Obesity_Type_III': 6\n",
        "                  }\n",
        "\n",
        "# Define a method for Cross validation. Here we are using StratifiedKFold.\n",
        "def cross_val_model(estimators, cv=skf, verbose=True):\n",
        "    '''\n",
        "        estimators : pipeline consisting preprocessing, encoder & model\n",
        "        cv : Method for cross-validation (default: StratifiedKFold)\n",
        "        verbose : print train/valid score (yes/no)\n",
        "    '''\n",
        "    # Data preparation\n",
        "    X = train.copy()  # Create a copy of the training data\n",
        "    y = X.pop(TARGET)  # Separate the target variable from X\n",
        "\n",
        "    # Initialize arrays to store predictions\n",
        "    y = y.map(target_mapping)  # Convert target variable values to integers using 'target_mapping'\n",
        "    test_predictions = np.zeros((len(test), 7))  # Initialize array to store predictions for test data\n",
        "    valid_predictions = np.zeros((len(X), 7))  # Initialize array to store predictions for validation data\n",
        "\n",
        "    # Cross-validation\n",
        "    val_scores, train_scores = [], []\n",
        "    for fold, (train_ind, valid_ind) in enumerate(skf.split(X, y)):\n",
        "        model = clone(estimators)  # Create a deep copy of the given estimator (model)\n",
        "\n",
        "        # Define train set\n",
        "        X_train = X.iloc[train_ind]\n",
        "        y_train = y.iloc[train_ind]\n",
        "\n",
        "        # Define valid set\n",
        "        X_valid = X.iloc[valid_ind]\n",
        "        y_valid = y.iloc[valid_ind]\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        model.fit(X_train, y_train)\n",
        "        if verbose:\n",
        "            print(\"-\" * 100)\n",
        "            print(f\"Fold: {fold}\")\n",
        "            print(f\"Train Accuracy Score: {accuracy_score(y_true=y_train, y_pred=model.predict(X_train))}\")\n",
        "            print(f\"Valid Accuracy Score: {accuracy_score(y_true=y_valid, y_pred=model.predict(X_valid))}\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "        # Calculate predictions for test data\n",
        "        test_predictions += model.predict_proba(test) / cv.get_n_splits()\n",
        "\n",
        "        # Save predictions for validation data\n",
        "        valid_predictions[valid_ind] = model.predict_proba(X_valid)\n",
        "\n",
        "        # Store validation scores\n",
        "        val_scores.append(accuracy_score(y_true=y_valid, y_pred=model.predict(X_valid)))\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Average Mean Accuracy Score: {np.array(val_scores).mean()}\")\n",
        "\n",
        "    return val_scores, valid_predictions, test_predictions\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.057135,
          "end_time": "2024-02-17T02:29:17.989144",
          "exception": false,
          "start_time": "2024-02-17T02:29:17.932009",
          "status": "completed"
        },
        "tags": [],
        "id": "w9e0zvNjk_lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Original & Synthetic Data\n",
        "# Combine the original data with additional provided data\n",
        "# Remove duplicates from the data to process it into a clean format\n",
        "# The processed data is used for training machine learning models.\n",
        "\n",
        "# Remove the 'id' column\n",
        "train.drop(['id'], axis=1, inplace=True)  # Remove the 'id' column from the training data\n",
        "test_ids = test['id']  # Save the 'id' column of the test dataset to test_ids for later use\n",
        "test.drop(['id'], axis=1, inplace=True)  # Also remove the 'id' column from the test dataset\n",
        "\n",
        "# Combine original and synthetic data\n",
        "train = pd.concat([train, train_org], axis=0)  # Use the 'pd.concat' function to combine the original training dataset with the additional provided synthetic dataset\n",
        "\n",
        "# Remove duplicates\n",
        "train = train.drop_duplicates()  # Remove duplicate rows from the combined dataset to maintain data consistency and eliminate unnecessary duplicate data\n",
        "\n",
        "# Reset the index\n",
        "train.reset_index(drop=True, inplace=True)  # After removing duplicates, reset the index; drop=True discards the old index without adding it as a new column\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.087979,
          "end_time": "2024-02-17T02:29:18.119515",
          "exception": false,
          "start_time": "2024-02-17T02:29:18.031536",
          "status": "completed"
        },
        "tags": [],
        "id": "RrRFkN0Pk_lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty dataframes to store scores, train/test predictions.\n",
        "# These three dataframes are used to systematically store and manage various results generated during the modeling process,\n",
        "# providing necessary information for analyzing the final prediction results.\n",
        "\n",
        "score_list, oof_list, predict_list = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "# score_list: Empty dataframe to store the scores of the models.\n",
        "# oof_list: Empty dataframe to store the predictions made by the model on the training/validation data.\n",
        "# predict_list: Empty dataframe to store the predictions made by the model on the test data.\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.050918,
          "end_time": "2024-02-17T02:29:18.21465",
          "exception": false,
          "start_time": "2024-02-17T02:29:18.163732",
          "status": "completed"
        },
        "tags": [],
        "id": "asGxNF8Ak_lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.042238,
          "end_time": "2024-02-17T02:29:18.297911",
          "exception": false,
          "start_time": "2024-02-17T02:29:18.255673",
          "status": "completed"
        },
        "tags": [],
        "id": "8ZbTFofGk_lX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style = \"font-size:120%\">Rather than focusing on a single model, in this competition it's better to combine predictions from many high performing models. In this notebook we will be training Four different type of models and will combine their predictions for final sub.</div>\n",
        "\n",
        "* [Random Forest Model](#rfc)\n",
        "* [LGBM Model](#lgbm)\n",
        "* [XGB Model](#xgb)\n",
        "* [Catboost Model](#cat)\n",
        "\n",
        "#In this notebook, we'll train four different types of models and combine their predictions for the final submission, rather than focusing on a single model. The four models are:\n",
        "\n",
        "* Random Forest model\n",
        "* LGBM model\n",
        "* XGB model\n",
        "* Catboost model\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.041325,
          "end_time": "2024-02-17T02:29:18.381854",
          "exception": false,
          "start_time": "2024-02-17T02:29:18.340529",
          "status": "completed"
        },
        "tags": [],
        "id": "LtaZ45mzk_lY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"rfC\"> </a>\n",
        "# Random Forest Model"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.042043,
          "end_time": "2024-02-17T02:29:18.464684",
          "exception": false,
          "start_time": "2024-02-17T02:29:18.422641",
          "status": "completed"
        },
        "tags": [],
        "id": "GFZ6qa2sk_lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Random Forest Model Pipeline\n",
        "# Connect data preprocessing steps and modeling steps using the make_pipeline function.\n",
        "\n",
        "RFC = make_pipeline(\n",
        "                        ExtractFeatures,  # Preprocessing step to extract new features from the data, such as calculating BMI\n",
        "                        MEstimateEncoder(cols=['Gender','family_history_with_overweight','FAVC','CAEC',\n",
        "                                           'SMOKE','SCC','CALC','MTRANS']),  # Step to encode categorical variables into numerical ones\n",
        "                       RandomForestClassifier(random_state=RANDOM_SEED)  # Specify the Random Forest classification model, setting random_state=RANDOM_SEED to ensure reproducibility of the model\n",
        "                    )\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.057105,
          "end_time": "2024-02-17T02:29:18.564689",
          "exception": false,
          "start_time": "2024-02-17T02:29:18.507584",
          "status": "completed"
        },
        "tags": [],
        "id": "btw3m6RLk_lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute Random Forest Pipeline\n",
        "# Execute the defined Random Forest pipeline and store predictions for training and test data.\n",
        "\n",
        "val_scores, val_predictions, test_predictions = cross_val_model(RFC)\n",
        "# Call cross_val_model(RFC) to perform cross-validation on the Random Forest pipeline (RFC).\n",
        "# This function returns scores for validation data (val_scores), predictions for validation data (val_predictions), and predictions for test data (test_predictions).\n",
        "\n",
        "# Save train/validation predictions\n",
        "# Store training/validation predictions\n",
        "\n",
        "for k, v in target_mapping.items():\n",
        "    oof_list[f\"rfc_{k}\"] = val_predictions[:, v]\n",
        "\n",
        "# Save test predictions\n",
        "# Store test predictions\n",
        "for k, v in target_mapping.items():\n",
        "    predict_list[f\"rfc_{k}\"] = test_predictions[:, v]\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 37.696297,
          "end_time": "2024-02-17T02:29:56.309067",
          "exception": false,
          "start_time": "2024-02-17T02:29:18.61277",
          "status": "completed"
        },
        "tags": [],
        "id": "iYUXSVtfk_lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"lgbm\"></a>\n",
        "# LGBM Model"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.042788,
          "end_time": "2024-02-17T02:29:56.395179",
          "exception": false,
          "start_time": "2024-02-17T02:29:56.352391",
          "status": "completed"
        },
        "tags": [],
        "id": "1C_2fDR4k_lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Optuna Function To Tune LGBM Model\n",
        "\n",
        "def lgbm_objective(trial):\n",
        "    params = {\n",
        "        'learning_rate' : trial.suggest_float('learning_rate', .001, .1, log = True),\n",
        "        'max_depth' : trial.suggest_int('max_depth', 2, 20),\n",
        "        'subsample' : trial.suggest_float('subsample', .5, 1),\n",
        "        'min_child_weight' : trial.suggest_float('min_child_weight', .1, 15, log = True),\n",
        "        'reg_lambda' : trial.suggest_float('reg_lambda', .1, 20, log = True),\n",
        "        'reg_alpha' : trial.suggest_float('reg_alpha', .1, 10, log = True),\n",
        "        'n_estimators' : 1000,\n",
        "        'random_state' : RANDOM_SEED,\n",
        "        'device_type' : \"gpu\",\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 10, 1000),\n",
        "\n",
        "        #'boosting_type' : 'dart',\n",
        "    }\n",
        "\n",
        "    optuna_model = make_pipeline(\n",
        "                                 ExtractFeatures,\n",
        "                                 MEstimateEncoder(cols=['Gender','family_history_with_overweight','FAVC','CAEC',\n",
        "                                           'SMOKE','SCC','CALC','MTRANS']),\n",
        "                                LGBMClassifier(**params,verbose=-1)\n",
        "                                )\n",
        "    val_scores, _, _ = cross_val_model(optuna_model,verbose = False)\n",
        "    return np.array(val_scores).mean()\n",
        "\n",
        "lgbm_study = optuna.create_study(direction = 'maximize',study_name=\"LGBM\")"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.055779,
          "end_time": "2024-02-17T02:29:56.493704",
          "exception": false,
          "start_time": "2024-02-17T02:29:56.437925",
          "status": "completed"
        },
        "tags": [],
        "id": "gBc6PNxOk_lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute LGBM Tuning, To Tune set `TUNE` to True (it will take a long time)\n",
        "TUNE = False\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "if TUNE:\n",
        "    lgbm_study.optimize(lgbm_objective, 50)\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.051093,
          "end_time": "2024-02-17T02:29:56.587397",
          "exception": false,
          "start_time": "2024-02-17T02:29:56.536304",
          "status": "completed"
        },
        "tags": [],
        "id": "MZc6CaS7k_lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_columns = train.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_columns.remove('NObeyesdad')"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.058309,
          "end_time": "2024-02-17T02:29:56.691094",
          "exception": false,
          "start_time": "2024-02-17T02:29:56.632785",
          "status": "completed"
        },
        "tags": [],
        "id": "xpXiOWaxk_lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style = \"font-size:120%\">LGBM parameters in next cell are taken from @moazeldsokyx notebook you may check his great work in this notebook:<br></div>\n",
        "\n",
        "https://www.kaggle.com/code/moazeldsokyx/pgs4e2-highest-score-lgbm-hyperparameter-tuning/notebook\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.044192,
          "end_time": "2024-02-17T02:29:56.778608",
          "exception": false,
          "start_time": "2024-02-17T02:29:56.734416",
          "status": "completed"
        },
        "tags": [],
        "id": "U8BbjfkQk_lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we defined LGBM Pipeline\n",
        "# Where we use One_Hot_Encoder, for categorical encoding\n",
        "# standard scaler for numerical column scaling\n",
        "\n",
        "\n",
        "params = {'learning_rate': 0.04325905707439143, 'max_depth': 4,\n",
        "          'subsample': 0.6115083405793659, 'min_child_weight': 0.43633356137010687,\n",
        "          'reg_lambda': 9.231766981717822, 'reg_alpha': 1.875987414096491, 'num_leaves': 373,\n",
        "          'n_estimators' : 1000,'random_state' : RANDOM_SEED, 'device_type' : \"gpu\",\n",
        "         }\n",
        "\n",
        "best_params = {\n",
        "    \"objective\": \"multiclass\",          # Objective function for the model\n",
        "    \"metric\": \"multi_logloss\",          # Evaluation metric\n",
        "    \"verbosity\": -1,                    # Verbosity level (-1 for silent)\n",
        "    \"boosting_type\": \"gbdt\",            # Gradient boosting type\n",
        "    \"random_state\": 42,       # Random state for reproducibility\n",
        "    \"num_class\": 7,                     # Number of classes in the dataset\n",
        "    'learning_rate': 0.030962211546832760,  # Learning rate for gradient boosting\n",
        "    'n_estimators': 500,                # Number of boosting iterations\n",
        "    'lambda_l1': 0.009667446568254372,  # L1 regularization term\n",
        "    'lambda_l2': 0.04018641437301800,   # L2 regularization term\n",
        "    'max_depth': 10,                    # Maximum depth of the trees\n",
        "    'colsample_bytree': 0.40977129346872643,  # Fraction of features to consider for each tree\n",
        "    'subsample': 0.9535797422450176,    # Fraction of samples to consider for each boosting iteration\n",
        "    'min_child_samples': 26             # Minimum number of data needed in a leaf\n",
        "}\n",
        "\n",
        "lgbm = make_pipeline(\n",
        "                        ColumnTransformer(\n",
        "                        transformers=[('num', StandardScaler(), numerical_columns),\n",
        "                                  ('cat', OneHotEncoder(handle_unknown=\"ignore\"), categorical_columns)]),\n",
        "                        LGBMClassifier(**best_params,verbose=-1)\n",
        "                    )"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.057371,
          "end_time": "2024-02-17T02:29:56.880242",
          "exception": false,
          "start_time": "2024-02-17T02:29:56.822871",
          "status": "completed"
        },
        "tags": [],
        "id": "zr-pQG0Vk_lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LGBM Model\n",
        "\n",
        "val_scores,val_predictions,test_predictions = cross_val_model(lgbm)\n",
        "\n",
        "for k,v in target_mapping.items():\n",
        "    oof_list[f\"lgbm_{k}\"] = val_predictions[:,v]\n",
        "\n",
        "for k,v in target_mapping.items():\n",
        "    predict_list[f\"lgbm_{k}\"] = test_predictions[:,v]\n",
        "\n",
        "#0.91420543252078"
      ],
      "metadata": {
        "papermill": {
          "duration": 153.445937,
          "end_time": "2024-02-17T02:32:30.369727",
          "exception": false,
          "start_time": "2024-02-17T02:29:56.92379",
          "status": "completed"
        },
        "tags": [],
        "id": "JXwnnp9Ek_lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"xgb\"></a>\n",
        "# XGB Model"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.044715,
          "end_time": "2024-02-17T02:32:30.46338",
          "exception": false,
          "start_time": "2024-02-17T02:32:30.418665",
          "status": "completed"
        },
        "tags": [],
        "id": "Dlaokhy_k_lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna study for XGB Model\n",
        "def xgb_objective(trial):\n",
        "    params = {\n",
        "        'grow_policy': trial.suggest_categorical('grow_policy', [\"depthwise\", \"lossguide\"]),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
        "        'gamma' : trial.suggest_float('gamma', 1e-9, 1.0),\n",
        "        'subsample': trial.suggest_float('subsample', 0.25, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.25, 1.0),\n",
        "        'max_depth': trial.suggest_int('max_depth', 0, 24),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 30),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 10.0, log=True),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 10.0, log=True),\n",
        "    }\n",
        "\n",
        "    params['booster'] = 'gbtree'\n",
        "    params['objective'] = 'multi:softmax'\n",
        "    params[\"device\"] = \"cuda\"\n",
        "    params[\"verbosity\"] = 0\n",
        "    params['tree_method'] = \"gpu_hist\"\n",
        "\n",
        "\n",
        "    optuna_model = make_pipeline(\n",
        "#                     ExtractFeatures,\n",
        "                    MEstimateEncoder(cols=['Gender','family_history_with_overweight','FAVC','CAEC',\n",
        "                                           'SMOKE','SCC','CALC','MTRANS']),\n",
        "                    XGBClassifier(**params,seed=RANDOM_SEED)\n",
        "                   )\n",
        "\n",
        "    val_scores, _, _ = cross_val_model(optuna_model,verbose = False)\n",
        "    return np.array(val_scores).mean()\n",
        "\n",
        "xgb_study = optuna.create_study(direction = 'maximize')\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.059915,
          "end_time": "2024-02-17T02:32:30.567884",
          "exception": false,
          "start_time": "2024-02-17T02:32:30.507969",
          "status": "completed"
        },
        "tags": [],
        "id": "YHrrfGmzk_lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tune using Optuna\n",
        "TUNE = False\n",
        "if TUNE:\n",
        "    xgb_study.optimize(xgb_objective, 50)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.052626,
          "end_time": "2024-02-17T02:32:30.665678",
          "exception": false,
          "start_time": "2024-02-17T02:32:30.613052",
          "status": "completed"
        },
        "tags": [],
        "id": "O2KC-Sd2k_lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGB Pipeline\n",
        "\n",
        "params = {\n",
        "    'n_estimators': 1312,\n",
        "    'learning_rate': 0.018279520260162645,\n",
        "    'gamma': 0.0024196354156454324,\n",
        "    'reg_alpha': 0.9025931173755949,\n",
        "    'reg_lambda': 0.06835667255875388,\n",
        "    'max_depth': 5,\n",
        "    'min_child_weight': 5,\n",
        "    'subsample': 0.883274050086088,\n",
        "    'colsample_bytree': 0.6579828557036317\n",
        "}\n",
        "# {'eta': 0.018387615982905264, 'max_depth': 29, 'subsample': 0.8149303101087905, 'colsample_bytree': 0.26750463604831476, 'min_child_weight': 0.5292380065098192, 'reg_lambda': 0.18952063379457604, 'reg_alpha': 0.7201451827004944}\n",
        "\n",
        "params = {'grow_policy': 'depthwise', 'n_estimators': 690,\n",
        "               'learning_rate': 0.31829021594473056, 'gamma': 0.6061120644431842,\n",
        "               'subsample': 0.9032243794829076, 'colsample_bytree': 0.44474031945048287,\n",
        "               'max_depth': 10, 'min_child_weight': 22, 'reg_lambda': 4.42638097284094,\n",
        "               'reg_alpha': 5.927900973354344e-07,'seed':RANDOM_SEED}\n",
        "\n",
        "best_params = {'grow_policy': 'depthwise', 'n_estimators': 982,\n",
        "               'learning_rate': 0.050053726931263504, 'gamma': 0.5354391952653927,\n",
        "               'subsample': 0.7060590452456204, 'colsample_bytree': 0.37939433412123275,\n",
        "               'max_depth': 23, 'min_child_weight': 21, 'reg_lambda': 9.150224029846654e-08,\n",
        "               'reg_alpha': 5.671063656994295e-08}\n",
        "best_params['booster'] = 'gbtree'\n",
        "best_params['objective'] = 'multi:softmax'\n",
        "best_params[\"device\"] = \"cuda\"\n",
        "best_params[\"verbosity\"] = 0\n",
        "best_params['tree_method'] = \"gpu_hist\"\n",
        "\n",
        "XGB = make_pipeline(\n",
        "#                     ExtractFeatures,\n",
        "#                     MEstimateEncoder(cols=['Gender','family_history_with_overweight','FAVC','CAEC',\n",
        "#                                            'SMOKE','SCC','CALC','MTRANS']),\n",
        "#                     FeatureDropper(['FAVC','FCVC']),\n",
        "#                     ColumnRounder,\n",
        "#                     ColumnTransformer(\n",
        "#                     transformers=[('num', StandardScaler(), numerical_columns),\n",
        "#                                   ('cat', OneHotEncoder(handle_unknown=\"ignore\"), categorical_columns)]),\n",
        "                    MEstimateEncoder(cols=['Gender','family_history_with_overweight','FAVC','CAEC',\n",
        "                                           'SMOKE','SCC','CALC','MTRANS']),\n",
        "                    XGBClassifier(**best_params,seed=RANDOM_SEED)\n",
        "                   )"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.057662,
          "end_time": "2024-02-17T02:32:30.768093",
          "exception": false,
          "start_time": "2024-02-17T02:32:30.710431",
          "status": "completed"
        },
        "tags": [],
        "id": "yCQ56ytmk_lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_scores,val_predictions,test_predictions = cross_val_model(XGB)\n",
        "\n",
        "for k,v in target_mapping .items():\n",
        "    oof_list[f\"xgb_{k}\"] = val_predictions[:,v]\n",
        "\n",
        "for k,v in target_mapping.items():\n",
        "    predict_list[f\"xgb_{k}\"] = test_predictions[:,v]\n",
        "\n",
        "# 0.90634942296329\n",
        "#0.9117093455898445 with rounder\n",
        "#0.9163506382522121"
      ],
      "metadata": {
        "papermill": {
          "duration": 80.93929,
          "end_time": "2024-02-17T02:33:51.753118",
          "exception": false,
          "start_time": "2024-02-17T02:32:30.813828",
          "status": "completed"
        },
        "tags": [],
        "id": "ESyfLDKDk_la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"cat\"></a>\n",
        "# Catboost Model\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.045308,
          "end_time": "2024-02-17T02:33:51.844344",
          "exception": false,
          "start_time": "2024-02-17T02:33:51.799036",
          "status": "completed"
        },
        "tags": [],
        "id": "bfNisFaNk_la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna Function For Catboost Model\n",
        "def cat_objective(trial):\n",
        "\n",
        "    params = {\n",
        "\n",
        "        'iterations': 1000,  # High number of estimators\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'depth': trial.suggest_int('depth', 3, 10),\n",
        "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 10.0),\n",
        "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
        "        'random_seed': RANDOM_SEED,\n",
        "        'verbose': False,\n",
        "        'task_type':\"GPU\"\n",
        "    }\n",
        "\n",
        "    cat_features = ['Gender','family_history_with_overweight','FAVC','FCVC','NCP',\n",
        "                'CAEC','SMOKE','CH2O','SCC','FAF','TUE','CALC','MTRANS']\n",
        "    optuna_model = make_pipeline(\n",
        "                        ExtractFeatures,\n",
        "#                         AgeRounder,\n",
        "#                         HeightRounder,\n",
        "#                         MEstimateEncoder(cols = raw_cat_cols),\n",
        "                        CatBoostClassifier(**params,cat_features=cat_features)\n",
        "                        )\n",
        "    val_scores,_,_ = cross_val_model(optuna_model,verbose = False)\n",
        "    return np.array(val_scores).mean()\n",
        "\n",
        "cat_study = optuna.create_study(direction = 'maximize')"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.058381,
          "end_time": "2024-02-17T02:33:51.94901",
          "exception": false,
          "start_time": "2024-02-17T02:33:51.890629",
          "status": "completed"
        },
        "tags": [],
        "id": "8unlRsj9k_la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'learning_rate': 0.13762007048684638, 'depth': 5,\n",
        "          'l2_leaf_reg': 5.285199432056192, 'bagging_temperature': 0.6029582154263095,\n",
        "         'random_seed': RANDOM_SEED,\n",
        "        'verbose': False,\n",
        "        'task_type':\"GPU\",\n",
        "         'iterations':1000}\n",
        "\n",
        "\n",
        "CB = make_pipeline(\n",
        "#                         ExtractFeatures,\n",
        "#                         AgeRounder,\n",
        "#                         HeightRounder,\n",
        "#                         MEstimateEncoder(cols = raw_cat_cols),\n",
        "#                         CatBoostEncoder(cols = cat_features),\n",
        "                        CatBoostClassifier(**params, cat_features=categorical_columns)\n",
        "                        )"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.057309,
          "end_time": "2024-02-17T02:33:52.052721",
          "exception": false,
          "start_time": "2024-02-17T02:33:51.995412",
          "status": "completed"
        },
        "tags": [],
        "id": "-FmPB5N8k_la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Catboost Model\n",
        "val_scores,val_predictions,test_predictions = cross_val_model(CB)\n",
        "for k,v in target_mapping.items():\n",
        "    oof_list[f\"cat_{k}\"] = val_predictions[:,v]\n",
        "\n",
        "for k,v in target_mapping.items():\n",
        "    predict_list[f\"cat_{k}\"] = test_predictions[:,v]\n",
        "\n",
        "# best 0.91179835368868 with extract features, n_splits = 10\n",
        "# best 0.9121046227778054 without extract features, n_splits = 10"
      ],
      "metadata": {
        "papermill": {
          "duration": 100.47477,
          "end_time": "2024-02-17T02:35:32.575002",
          "exception": false,
          "start_time": "2024-02-17T02:33:52.100232",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-04T08:05:36.070523Z",
          "iopub.execute_input": "2024-03-04T08:05:36.070902Z",
          "iopub.status.idle": "2024-03-04T08:05:36.110046Z",
          "shell.execute_reply.started": "2024-03-04T08:05:36.070872Z",
          "shell.execute_reply": "2024-03-04T08:05:36.108663Z"
        },
        "trusted": true,
        "id": "dIrFvG47k_la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.046499,
          "end_time": "2024-02-17T02:35:32.66893",
          "exception": false,
          "start_time": "2024-02-17T02:35:32.622431",
          "status": "completed"
        },
        "tags": [],
        "id": "NlH8WU1Rk_la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# skf = StratifiedKFold(n_splits=5)\n",
        "weights = {\"rfc_\": 0,\n",
        "           \"lgbm_\": 3,\n",
        "           \"xgb_\": 1,\n",
        "           \"cat_\": 0}\n",
        "\n",
        "tmp = oof_list.copy()\n",
        "for k, v in target_mapping.items():\n",
        "    tmp[f\"{k}\"] = (weights['rfc_'] * tmp[f\"rfc_{k}\"] +\n",
        "                   weights['lgbm_'] * tmp[f\"lgbm_{k}\"] +\n",
        "                   weights['xgb_'] * tmp[f\"xgb_{k}\"] +\n",
        "                   weights['cat_'] * tmp[f\"cat_{k}\"])\n",
        "tmp['pred'] = tmp[target_mapping.keys()].idxmax(axis=1)\n",
        "tmp['label'] = train[TARGET]\n",
        "\n",
        "ensemble_accuracy = accuracy_score(train[TARGET], tmp['pred'])\n",
        "\n",
        "cm = confusion_matrix(y_true=tmp['label'].map(target_mapping),\n",
        "                      y_pred=tmp['pred'].map(target_mapping),\n",
        "                      normalize='true')\n",
        "\n",
        "cm = cm.round(2)\n",
        "plt.figure(figsize=(8, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=target_mapping.keys(),\n",
        "                              cmap='Blues')  #    \n",
        "disp.plot(xticks_rotation=50)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Ensemble Accuracy Score: {ensemble_accuracy}\")\n",
        "\n",
        "\"\"\"   BEST     \"\"\"\n",
        "\n",
        "# Best LB [0,1,0,0]\n",
        "# Average Train Score:0.9142044335854003\n",
        "# Average Valid Score:0.91420543252078\n",
        "\n",
        "# Best CV [1,3, 1,1]\n",
        "# Average Train Score:0.9168308163711971\n",
        "# Average Valid Score:0.9168308163711971\n",
        "# adding orignal data improves score\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.49924,
          "end_time": "2024-02-17T02:35:33.21618",
          "exception": false,
          "start_time": "2024-02-17T02:35:32.71694",
          "status": "completed"
        },
        "tags": [],
        "id": "td1iX0f_k_la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Submission"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.048702,
          "end_time": "2024-02-17T02:35:33.313163",
          "exception": false,
          "start_time": "2024-02-17T02:35:33.264461",
          "status": "completed"
        },
        "tags": [],
        "id": "NkDTHUqrk_lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.058367,
          "end_time": "2024-02-17T02:35:33.420341",
          "exception": false,
          "start_time": "2024-02-17T02:35:33.361974",
          "status": "completed"
        },
        "tags": [],
        "id": "3adjm0d-k_lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in target_mapping.items():\n",
        "    predict_list[f\"{k}\"] = (weights['rfc_']*predict_list[f\"rfc_{k}\"]+\n",
        "                            weights['lgbm_']*predict_list[f\"lgbm_{k}\"]+\n",
        "                            weights['xgb_']*predict_list[f\"xgb_{k}\"]+\n",
        "                            weights['cat_']*predict_list[f\"cat_{k}\"])\n",
        "\n",
        "final_pred = predict_list[target_mapping.keys()].idxmax(axis = 1)\n",
        "\n",
        "sample_sub[TARGET] = final_pred\n",
        "sample_sub.to_csv(\"submission.csv\",index=False)\n",
        "sample_sub\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.107928,
          "end_time": "2024-02-17T02:35:33.576511",
          "exception": false,
          "start_time": "2024-02-17T02:35:33.468583",
          "status": "completed"
        },
        "tags": [],
        "id": "Iq3xDyd5k_lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feel free to fork and test different things. some of the things you may try:\n",
        "* Try Different weights and see how CV & LB changes. weights = {'rfc_': 0.0, 'lgbm_': 1.0, 'xgb_': 0.0, 'cat_': 0.0} gives best LB. you may try that first\n",
        "* In this Notebook we used `StandardScaler`, next we can try `Log & MinMax Scaler transformer`\n",
        "* Define new features in `extract_feature` function by combining different features\n",
        "* Tune the models again using Optuna\n",
        "* In this notebook we are using weighted average, next we may use a linear model to combine the predictions.\n",
        "\n",
        "**This notebook is inspired by the awesome work of Iqbal Syah Akbar. I highly recommend to check his work.**\n",
        "\n",
        "https://www.kaggle.com/code/iqbalsyahakbar/ps4e1-3rd-place-solution\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Happy Coding !!**\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.051039,
          "end_time": "2024-02-17T02:35:33.680698",
          "exception": false,
          "start_time": "2024-02-17T02:35:33.629659",
          "status": "completed"
        },
        "tags": [],
        "id": "k3KNYeGak_ld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  "
      ],
      "metadata": {
        "id": "-Hi1GkOClMaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os;\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "os.listdir('/kaggle/input')\n",
        "os.environ['PYTHONHASHSEED'] = '51'\n",
        "rn.seed(89)\n",
        "tf.random.set_seed(40)"
      ],
      "metadata": {
        "id": "B6uA_WbClPxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Prameters for Reproduciblity\n",
        "pd.set_option(\"display.max_rows\",100)\n",
        "FILE_PATH = \"/kaggle/input/playground-series-s4e2/\"\n",
        "TARGET = \"NObeyesdad\"\n",
        "n_splits = 9\n",
        "RANDOM_SEED = 73"
      ],
      "metadata": {
        "id": "LsBFA_GylRhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "YWS1niNhlUE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load all data\n",
        "train = pd.read_csv(os.path.join(FILE_PATH, \"train.csv\"))\n",
        "test = pd.read_csv(os.path.join(FILE_PATH, \"test.csv\"))\n",
        "sample_sub = pd.read_csv(os.path.join(FILE_PATH, \"sample_submission.csv\"))\n",
        "train_org = pd.read_csv(\"/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv\")"
      ],
      "metadata": {
        "id": "StPyjLX6lSrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numeric data\n",
        "raw_num_cols = list(train.select_dtypes(\"float\").columns)\n",
        "# categorical data\n",
        "raw_cat_cols = list(train.columns.drop(raw_num_cols+[TARGET]))"
      ],
      "metadata": {
        "id": "APxCfZjTlXjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering & Processing\n",
        "\n",
        "It defines a series of functions and custom transformers for preprocessing data before feeding it into a machine learning model. The age_rounder and height_rounder functions multiply the 'Age' and 'Height' column values by 100 and convert them to uint16 data type to round them, potentially enhancing model performance. The extract_features function calculates the BMI(Body Mass Index) using 'Weight' and 'Height' columns and adds it as a new feature to the dataframe. Additionally, the col_rounder function rounds specific columns ('FCVC', \"NCP\", \"CH2O\", \"FAF\", \"TUE\") and converts them to integers. Each transformation is encapsulated within a FunctionTransformer object. Lastly, the FeatureDropper class is a custom transformer that drops specified columns from the input DataFrame, allowing for further customization of the data preprocessing pipeline."
      ],
      "metadata": {
        "id": "0Gb2X-pKlZov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Round age values for potential model performance improvement\n",
        "def age_rounder(x):\n",
        "    # Make a copy of the input dataframe\n",
        "    x_copy = x.copy()\n",
        "    # Multiply 'Age' column values by 100 and convert to uint16 for rounding age\n",
        "    x_copy['Age'] = (x_copy['Age']*100).astype(np.uint16)\n",
        "    return x_copy\n",
        "\n",
        "# Round height values for potential model performance improvement\n",
        "def height_rounder(x):\n",
        "    # Make a copy of the input dataframe\n",
        "    x_copy = x.copy()\n",
        "    # Multiply 'Height' column values by 100 and convert to uint16 for rounding height\n",
        "    x_copy['Height'] = (x_copy['Height']*100).astype(np.uint16)\n",
        "    return x_copy\n",
        "\n",
        "# Extract new features: BMI and PseudoTarget\n",
        "def extract_features(x):\n",
        "    # Make a copy of the input dataframe\n",
        "    x_copy = x.copy()\n",
        "    # Calculate BMI using 'Weight' and 'Height' columns\n",
        "    x_copy['BMI'] = (x_copy['Weight']/x_copy['Height']**2)\n",
        "#     x_copy['PseudoTarget'] = pd.cut(x_copy['BMI'], bins=[0,18.4,24.9,29,34.9,39.9,100], labels=[0,1,2,3,4,5])  # Define 'PseudoTarget' based on BMI categories\n",
        "    return x_copy\n",
        "\n",
        "# Round specific columns' values\n",
        "def col_rounder(x):\n",
        "    # Make a copy of the input dataframe\n",
        "    x_copy = x.copy()\n",
        "    # Define columns to round\n",
        "    cols_to_round = ['FCVC',\"NCP\",\"CH2O\",\"FAF\",\"TUE\"]\n",
        "    # Round values in specified columns\n",
        "    for col in cols_to_round:\n",
        "        x_copy[col] = round(x_copy[col])\n",
        "        x_copy[col] = x_copy[col].astype('int')\n",
        "    return x_copy\n",
        "\n",
        "# Create FunctionTransformer objects for each transformation\n",
        "AgeRounder = FunctionTransformer(age_rounder)\n",
        "HeightRounder = FunctionTransformer(height_rounder)\n",
        "ExtractFeatures = FunctionTransformer(extract_features)\n",
        "ColumnRounder = FunctionTransformer(col_rounder)"
      ],
      "metadata": {
        "id": "TXY8tMyKlY3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom transformer to drop specified columns\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class FeatureDropper(BaseEstimator, TransformerMixin):\n",
        "    # Initialize FeatureDropper with columns to drop\n",
        "        def __init__(self, cols):\n",
        "        self.cols = cols\n",
        "\n",
        "    # Fit method (no action needed)\n",
        "    # TransformerMixin requires Fit method\n",
        "    def fit(self, x, y):\n",
        "        return self\n",
        "\n",
        "    # Transform method to drop specified columns from input DataFrame\n",
        "    def transform(self, x):\n",
        "        # Drop specified columns from the input DataFrame\n",
        "        return x.drop(self.cols, axis=1)\n"
      ],
      "metadata": {
        "id": "27rsngrtlfqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define cross_val_model(used to train and validate all the models)\n",
        "\n",
        "It performs cross-validation of machine learning models using Stratified K-Fold technique. It begins by encoding target values into integers and defining the Stratified K-Fold object. The cross_val_model function then conducts model training and validation within each fold, storing the predictions and accuracy scores. After training the models, it prints the training and validation accuracy scores for each fold if verbose mode is enabled. Finally, it combines original and synthetic data, drops unnecessary columns, and concatenates them before resetting the index and initializing empty dataframes to store scores and predictions. Overall, this code provides a robust framework for evaluating machine learning models through cross-validation."
      ],
      "metadata": {
        "id": "FAThFbnklhzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validate models using Stratified K-Fold\n",
        "\n",
        "# Encode target values as integers\n",
        "target_mapping = {\n",
        "                  'Insufficient_Weight':0,\n",
        "                  'Normal_Weight':1,\n",
        "                  'Overweight_Level_I':2,\n",
        "                  'Overweight_Level_II':3,\n",
        "                  'Obesity_Type_I':4,\n",
        "                  'Obesity_Type_II':5 ,\n",
        "                  'Obesity_Type_III':6\n",
        "                  }\n",
        "\n",
        "# Method for cross-validation using StratifiedKFold by default\n",
        "skf = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "def cross_val_model(estimators, cv=skf, verbose=True):\n",
        "    '''\n",
        "        estimators : pipeline consists preprocessing, encoder & model\n",
        "        cv : Method for cross validation (default: StratifiedKfold)\n",
        "        verbose : print train/valid score (yes/no)\n",
        "    '''\n",
        "\n",
        "    # Copy the original data\n",
        "    X = train.copy()\n",
        "    # Extract target variable and remove it from training data\n",
        "    y = X.pop(TARGET)\n",
        "\n",
        "    # Map target variable values to integers\n",
        "    y = y.map(target_mapping)\n",
        "\n",
        "\t\t# Initialize arrays to store predictions\n",
        "    test_predictions = np.zeros((len(test),7))\n",
        "    valid_predictions = np.zeros((len(X),7))\n",
        "\n",
        "    # Initialize lists to store scores\n",
        "    val_scores, train_scores = [],[]\n",
        "\n",
        "    # Iterate over each fold\n",
        "    for fold, (train_ind, valid_ind) in enumerate(skf.split(X,y)):\n",
        "        # Clone the model to ensure a fresh instance for each fold\n",
        "        model = clone(estimators)\n",
        "\n",
        "        # Define train set\n",
        "        X_train = X.iloc[train_ind]\n",
        "        y_train = y.iloc[train_ind]\n",
        "        # Define valid set\n",
        "        X_valid = X.iloc[valid_ind]\n",
        "        y_valid = y.iloc[valid_ind]\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Print training and validation accuracies if verbose is True\n",
        "        if verbose:\n",
        "            print(\"-\" * 100)\n",
        "            print(f\"Fold: {fold}\")\n",
        "            print(f\"Train Accuracy Score:-{accuracy_score(y_true=y_train,y_pred=model.predict(X_train))}\")\n",
        "            print(f\"Valid Accuracy Score:-{accuracy_score(y_true=y_valid,y_pred=model.predict(X_valid))}\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "        # Store predictions for test and validation data\n",
        "        test_predictions += model.predict_proba(test)/cv.get_n_splits()\n",
        "        valid_predictions[valid_ind] = model.predict_proba(X_valid)\n",
        "\n",
        "        # Calculate validation scores and store them\n",
        "        val_scores.append(accuracy_score(y_true=y_valid,y_pred=model.predict(X_valid)))\n",
        "\n",
        "    # Print average mean accuracy score if verbose is True\n",
        "    if verbose:\n",
        "        print(f\"Average Mean Accuracy Score:- {np.array(val_scores).mean()}\")\n",
        "\n",
        "    # Return validation scores and predictions, and test predictions\n",
        "    return val_scores, valid_predictions, test_predictions"
      ],
      "metadata": {
        "id": "9CQJ4VvZlly5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Orignal & Synthetic Data\n",
        "# Drop 'id' column from the training data\n",
        "train.drop(['id'],axis = 1, inplace = True)\n",
        "\n",
        "# Store test IDs before dropping 'id' column from the test data\n",
        "test_ids = test['id']\n",
        "# Drop 'id' column from the test data\n",
        "test.drop(['id'],axis = 1, inplace=True)\n",
        "\n",
        "# Combine original and synthetic data by concatenating them along axis 0 (rows)\n",
        "train = pd.concat([train,train_org],axis = 0)\n",
        "# Remove duplicate rows from the combined data\n",
        "train = train.drop_duplicates()\n",
        "# Reset index after removing duplicates\n",
        "train.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "NGV3IXTBlnyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty dataframes to store scores & train/test predictions\n",
        "score_list, oof_list, predict_list = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()"
      ],
      "metadata": {
        "id": "S7kX_WGDlqbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest Model\n",
        "\n",
        "It defines a pipeline for a Random Forest Classifier model. The pipeline first preprocesses the data by extracting features and encoding categorical features using the M-Estimate Encoder. Then, it includes a Random Forest Classifier model with a specified random state. The pipeline is executed using the cross_val_model function, which performs cross-validation to evaluate the model's performance. The validation set predictions for each class are stored in separate columns in the 'oof_list' dataframe, while the test set predictions are stored similarly in the 'predict_list' dataframe. This pipeline facilitates the preprocessing and evaluation of the Random Forest model, while also managing predictions efficiently."
      ],
      "metadata": {
        "id": "-YGpk6yJls0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Random Forest Model Pipeline\n",
        "RFC = make_pipeline(\n",
        "                    # Extract features\n",
        "                    ExtractFeatures,\n",
        "                    # Encode categorical features using M-Estimate Encoder\n",
        "                    MEstimateEncoder(cols=['Gender','family_history_with_overweight','FAVC','CAEC',\n",
        "                                           'SMOKE','SCC','CALC','MTRANS']),\n",
        "                    # Random Forest Classifier\n",
        "                    RandomForestClassifier(random_state=RANDOM_SEED)\n",
        "                    )"
      ],
      "metadata": {
        "id": "vbcC8qGHlsH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute Random Forest Pipeline\n",
        "val_scores,val_predictions,test_predictions = cross_val_model(RFC)\n",
        "\n",
        "# Save train predictions in dataframes\n",
        "for k,v in target_mapping.items():\n",
        "    # Save validation set predictions for each class in a separate column in the 'oof_list' dataframe\n",
        "    oof_list[f\"rfc_{k}\"] = val_predictions[:,v]\n",
        "\n",
        "# Save test predictions in dataframes\n",
        "for k,v in target_mapping.items():\n",
        "    # Save test set predictions for each class in a separate column in the 'predict_list' dataframe\n",
        "    predict_list[f\"rfc_{k}\"] = test_predictions[:,v]"
      ],
      "metadata": {
        "id": "CJ__5ObDlw6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LGBM Model\n",
        "\n",
        "It tunes and trains a LightGBM model using Optuna. First, it defines the objective function lgbm_objective for Optuna to optimize hyperparameters such as learning rate, max depth, subsample ratio, etc., through cross-validation. The study lgbm_study is then created to maximize the validation score. Additionally, it initializes the model with default hyperparameters and executes tuning if TUNE is set to True. The best parameters obtained from tuning or predefined parameters are defined. Later, a LightGBM pipeline is constructed, including data preprocessing steps such as standard scaling for numerical features and one-hot encoding for categorical features. Finally, the model is trained using cross-validation, and the predictions for both the validation and test sets are stored in respective dataframes. This code provides a comprehensive framework for optimizing and training LightGBM models for multi-class classification tasks."
      ],
      "metadata": {
        "id": "tXWaEFdNly4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Optuna function to tune LGBM model\n",
        "def lgbm_objective(trial):\n",
        "    # Define hyperparameters to be tuned\n",
        "    params = {\n",
        "        'learning_rate' : trial.suggest_float('learning_rate', .001, .1, log = True),  # Learning rate tuning\n",
        "        'max_depth' : trial.suggest_int('max_depth', 2, 20),  # Max depth tuning\n",
        "        'subsample' : trial.suggest_float('subsample', .5, 1),  # Subsample ratio tuning\n",
        "        'min_child_weight' : trial.suggest_float('min_child_weight', .1, 15, log = True),  # Min child weight tuning\n",
        "        'reg_lambda' : trial.suggest_float('reg_lambda', .1, 20, log = True),  # L2 regularization tuning\n",
        "        'reg_alpha' : trial.suggest_float('reg_alpha', .1, 10, log = True),  # L1 regularization tuning\n",
        "        'n_estimators' : 1000,  # Number of estimators\n",
        "        'random_state' : RANDOM_SEED,  # Random state for reproducibility\n",
        "        'device_type' : \"gpu\",  # Specify device type (GPU)\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 10, 1000),  # Number of leaves tuning\n",
        "#         'boosting_type' : 'dart',  # Boosting type (DART)\n",
        "    }\n",
        "\n",
        "    # Construct LightGBM model pipeline with Optuna suggested hyperparameters\n",
        "    optuna_model = make_pipeline(\n",
        "                                 # Extract features\n",
        "                                 ExtractFeatures,\n",
        "                                 # Encode categorical features\n",
        "                                 MEstimateEncoder(cols=['Gender','family_history_with_overweight','FAVC','CAEC',\n",
        "                                           'SMOKE','SCC','CALC','MTRANS']),\n",
        "                                 # LightGBM classifier with tuned hyperparameters\n",
        "                                 LGBMClassifier(**params,verbose=-1)\n",
        "                                 )\n",
        "\n",
        "    # Perform cross-validation and obtain validation scores\n",
        "    val_scores, _, _ = cross_val_model(optuna_model,verbose = False)\n",
        "\n",
        "    # Return the mean of validation scores as the optimization objective\n",
        "    return np.array(val_scores).mean()\n",
        "\n",
        "# Create an Optuna study for maximizing the validation score\n",
        "lgbm_study = optuna.create_study(direction = 'maximize',study_name=\"LGBM\")"
      ],
      "metadata": {
        "id": "m6a3NTxYl1AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute LGBM Tuning, To Tune set `TUNE` to True (it will take a long time)\n",
        "TUNE = False\n",
        "\n",
        "# Ignore warning messages\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "if TUNE:\n",
        "    # Perform optimization with 50 trials\n",
        "    lgbm_study.optimize(lgbm_objective, 50)"
      ],
      "metadata": {
        "id": "uQ9IDoLBl3RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of numerical columns\n",
        "numerical_columns = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Get list of categorical columns\n",
        "categorical_columns = train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Remove target column from categorical columns\n",
        "categorical_columns.remove('NObeyesdad')"
      ],
      "metadata": {
        "id": "LRmeRVTtl59a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LGBM parameters in next cell are taken from @moazeldsokyx notebook you may check his great work in this notebook:\n",
        " https://www.kaggle.com/code/moazeldsokyx/pgs4e2-highest-score-lgbm-hyperparameter-tuning/notebook"
      ],
      "metadata": {
        "id": "UWQBRqUnl7OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters for LGBM model\n",
        "params = {'learning_rate': 0.04325905707439143, 'max_depth': 4,\n",
        "          'subsample': 0.6115083405793659, 'min_child_weight': 0.43633356137010687,\n",
        "          'reg_lambda': 9.231766981717822, 'reg_alpha': 1.875987414096491, 'num_leaves': 373,\n",
        "          'n_estimators' : 1000,'random_state' : RANDOM_SEED, 'device_type' : \"gpu\",\n",
        "         }\n",
        "\n",
        "# Define best parameters obtained from tuning\n",
        "best_params = {\n",
        "    \"objective\": \"multiclass\",          # Objective function for the model\n",
        "    \"metric\": \"multi_logloss\",          # Evaluation metric\n",
        "    \"verbosity\": -1,                    # Verbosity level (-1 for silent)\n",
        "    \"boosting_type\": \"gbdt\",            # Gradient boosting type\n",
        "    \"random_state\": 42,                 # Random state for reproducibility\n",
        "    \"num_class\": 7,                     # Number of classes in the dataset\n",
        "    'learning_rate': 0.030962211546832760,  # Learning rate for gradient boosting\n",
        "    'n_estimators': 500,                # Number of boosting iterations\n",
        "    'lambda_l1': 0.009667446568254372,  # L1 regularization term\n",
        "    'lambda_l2': 0.04018641437301800,   # L2 regularization term\n",
        "    'max_depth': 10,                    # Maximum depth of the trees\n",
        "    'colsample_bytree': 0.40977129346872643,  # Fraction of features to consider for each tree\n",
        "    'subsample': 0.9535797422450176,    # Fraction of samples to consider for each boosting iteration\n",
        "    'min_child_samples': 26             # Minimum number of data needed in a leaf\n",
        "}\n",
        "\n",
        "# Define LGBM pipeline\n",
        "lgbm = make_pipeline(\n",
        "                        ColumnTransformer(\n",
        "                        transformers=[('num', StandardScaler(), numerical_columns),\n",
        "                                  ('cat', OneHotEncoder(handle_unknown=\"ignore\"), categorical_columns)]),\n",
        "                        LGBMClassifier(**best_params,verbose=-1)\n",
        "                    )"
      ],
      "metadata": {
        "id": "6ZJ3WSNBl9QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LGBM Model\n",
        "val_scores,val_predictions,test_predictions = cross_val_model(lgbm)\n",
        "\n",
        "# Save train predictions in DataFrame\n",
        "for k,v in target_mapping.items():\n",
        "    oof_list[f\"lgbm_{k}\"] = val_predictions[:,v]\n",
        "\n",
        "# Save test predictions in DataFrame\n",
        "for k,v in target_mapping.items():\n",
        "    predict_list[f\"lgbm_{k}\"] = test_predictions[:,v]"
      ],
      "metadata": {
        "id": "2xkwyjuNl_xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGB Model\n",
        "\n",
        "It optimizes and trains an XGBoost model using Optuna. Firstly, an objective function xgb_objective is defined to optimize the hyperparameters of the XGBoost model. This function utilizes Optuna to search for hyperparameter values within specified ranges and returns the mean validation score through cross-validation. Then, an Optuna study xgb_study is created to find the hyperparameter configuration that maximizes the validation score. If the TUNE variable is set to True, hyperparameter tuning is performed. Initial and optimized hyperparameters are defined, and an XGBoost pipeline is constructed. Lastly, the model is trained using cross-validation, and predictions for the validation and test sets are stored in their respective dataframes. This code provides a comprehensive framework for tuning and training an XGBoost model."
      ],
      "metadata": {
        "id": "8KhW9fYnmAQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna study for XGB Model\n",
        "# Define the objective function for the Optuna study to optimize XGBoost hyperparameters\n",
        "def xgb_objective(trial):\n",
        "    # Define hyperparameters to be tuned\n",
        "    params = {\n",
        "        'grow_policy': trial.suggest_categorical('grow_policy', [\"depthwise\", \"lossguide\"]),  # Choose between 'depthwise' and 'lossguide' for tree growth policy\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 2000),  # Number of boosting iterations\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),  # Learning rate\n",
        "        'gamma' : trial.suggest_float('gamma', 1e-9, 1.0),  # Minimum loss reduction required to make a further partition on a leaf node\n",
        "        'subsample': trial.suggest_float('subsample', 0.25, 1.0),  # Subsample ratio of training instances\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.25, 1.0),  # Subsample ratio of columns when constructing each tree\n",
        "        'max_depth': trial.suggest_int('max_depth', 0, 24),  # Maximum depth of the tree\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 30),  # Minimum sum of instance weight needed in a child\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 10.0, log=True),  # L2 regularization term on weights\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 10.0, log=True),  # L1 regularization term on weights\n",
        "    }\n",
        "\n",
        "    # Additional parameters for XGBoost model\n",
        "    params['booster'] = 'gbtree'  # Use tree-based models\n",
        "    params['objective'] = 'multi:softmax'  # Objective function for multiclass classification\n",
        "    params[\"device\"] = \"cuda\"  # Use GPU for computation\n",
        "    params[\"verbosity\"] = 0  # Verbosity level\n",
        "    params['tree_method'] = \"gpu_hist\"  # Method for tree construction using GPU\n",
        "\n",
        "\n",
        "    # Construct XGB model pipeline with suggested hyperparameters\n",
        "    optuna_model = make_pipeline(\n",
        "#                     ExtractFeatures,  # Extract features from data\n",
        "                    # Encode categorical features using M-Estimate Encoder\n",
        "                    MEstimateEncoder(cols=['Gender','family_history_with_overweight','FAVC','CAEC',\n",
        "                                           'SMOKE','SCC','CALC','MTRANS']),\n",
        "                    # XGBoost classifier with tuned hyperparameters\n",
        "                    XGBClassifier(**params,seed=RANDOM_SEED)\n",
        "                   )\n",
        "\n",
        "    # Perform cross-validation with the model pipeline and return mean validation scores\n",
        "    val_scores, _, _ = cross_val_model(optuna_model,verbose = False)\n",
        "    return np.array(val_scores).mean()\n",
        "\n",
        "# Create an Optuna study for maximizing validation scores\n",
        "xgb_study = optuna.create_study(direction = 'maximize')"
      ],
      "metadata": {
        "id": "jmWxiywzmCWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute XGB Tuning, To Tune set `TUNE` to True (it will take a long time)\n",
        "TUNE = False\n",
        "\n",
        "if TUNE:\n",
        "    # Perform optimization with 50 trials\n",
        "    xgb_study.optimize(xgb_objective, 50)"
      ],
      "metadata": {
        "id": "EzI1oN4hmD_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial hyperparameters for the XGBoost model\n",
        "params = {\n",
        "    'n_estimators': 1312,\n",
        "    'learning_rate': 0.018279520260162645,\n",
        "    'gamma': 0.0024196354156454324,\n",
        "    'reg_alpha': 0.9025931173755949,\n",
        "    'reg_lambda': 0.06835667255875388,\n",
        "    'max_depth': 5,\n",
        "    'min_child_weight': 5,\n",
        "    'subsample': 0.883274050086088,\n",
        "    'colsample_bytree': 0.6579828557036317\n",
        "}\n",
        "# {'eta': 0.018387615982905264, 'max_depth': 29, 'subsample': 0.8149303101087905, 'colsample_bytree': 0.26750463604831476, 'min_child_weight': 0.5292380065098192, 'reg_lambda': 0.18952063379457604, 'reg_alpha': 0.7201451827004944}\n",
        "\n",
        "# Updated hyperparameters for the XGBoost model\n",
        "params = {'grow_policy': 'depthwise', 'n_estimators': 690,\n",
        "               'learning_rate': 0.31829021594473056, 'gamma': 0.6061120644431842,\n",
        "               'subsample': 0.9032243794829076, 'colsample_bytree': 0.44474031945048287,\n",
        "               'max_depth': 10, 'min_child_weight': 22, 'reg_lambda': 4.42638097284094,\n",
        "               'reg_alpha': 5.927900973354344e-07,'seed':RANDOM_SEED}\n",
        "\n",
        "# Best hyperparameters found for the XGBoost model\n",
        "best_params = {'grow_policy': 'depthwise', 'n_estimators': 982,\n",
        "               'learning_rate': 0.050053726931263504, 'gamma': 0.5354391952653927,\n",
        "               'subsample': 0.7060590452456204, 'colsample_bytree': 0.37939433412123275,\n",
        "               'max_depth': 23, 'min_child_weight': 21, 'reg_lambda': 9.150224029846654e-08,\n",
        "               'reg_alpha': 5.671063656994295e-08}\n",
        "\n",
        "# Adding additional parameters to the best_params dictionary\n",
        "best_params['booster'] = 'gbtree'\n",
        "best_params['objective'] = 'multi:softmax'\n",
        "best_params[\"device\"] = \"cuda\"\n",
        "best_params[\"verbosity\"] = 0\n",
        "best_params['tree_method'] = \"gpu_hist\"\n",
        "\n",
        "# XGBoost pipeline\n",
        "XGB = make_pipeline(\n",
        "                    # Feature encoding and XGBoost classifier\n",
        "#                     ExtractFeatures,  # Extract features from data\n",
        "#                     MEstimateEncoder(cols=['Gender','family_history_with_overweight','FAVC','CAEC',\n",
        "#                                            'SMOKE','SCC','CALC','MTRANS']),  # Encode categorical features\n",
        "#                     FeatureDropper(['FAVC','FCVC']),  # Drop features from data\n",
        "#                     ColumnRounder,  # Round column values\n",
        "#                     ColumnTransformer(\n",
        "#                     transformers=[('num', StandardScaler(), numerical_columns),\n",
        "#                                   ('cat', OneHotEncoder(handle_unknown=\"ignore\"), categorical_columns)]),\n",
        "                    # Encode categorical features using M-Estimate Encoder\n",
        "                    MEstimateEncoder(cols=['Gender','family_history_with_overweight','FAVC','CAEC',\n",
        "                                           'SMOKE','SCC','CALC','MTRANS']),\n",
        "                    # XGBoost classifier with tuned hyperparameters\n",
        "                    XGBClassifier(**best_params,seed=RANDOM_SEED)\n",
        "                   )"
      ],
      "metadata": {
        "id": "_9HYDcZ0mGUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validate XGBoost model and get validation scores and predictions\n",
        "val_scores,val_predictions,test_predictions = cross_val_model(XGB)\n",
        "\n",
        "# Store validation predictions in respective categories\n",
        "for k,v in target_mapping .items():\n",
        "    oof_list[f\"xgb_{k}\"] = val_predictions[:,v]\n",
        "\n",
        "# Store test predictions in respective categories\n",
        "for k,v in target_mapping.items():\n",
        "    predict_list[f\"xgb_{k}\"] = test_predictions[:,v]"
      ],
      "metadata": {
        "id": "eqMY6NqPmHH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Catboost Model\n",
        "\n",
        "It defines an Optuna objective function, cat_objective, to optimize hyperparameters for the CatBoost model. The function explores a range of hyperparameters such as learning rate, tree depth, L2 regularization coefficient, and bagging temperature using Optuna's suggest methods. It constructs a CatBoost pipeline with the optimized parameters and performs cross-validation to evaluate the model's performance. The CatBoost pipeline includes feature extraction, categorical encoding, and CatBoost classifier instantiation. After defining the objective function, an Optuna study object cat_study is created to maximize the objective function. Additionally, a set of initial hyperparameters for the CatBoost model is specified. Finally, the CatBoost model is trained using cross-validation, and the validation and test predictions are stored in respective dataframes for further analysis. This code provides a comprehensive framework for optimizing and training CatBoost models."
      ],
      "metadata": {
        "id": "MOv2jo1JmIlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna Function For Catboost Model\n",
        "def cat_objective(trial):\n",
        "    # Define parameters to be optimized\n",
        "    params = {\n",
        "        'iterations': 1000,  # Number of boosting iterations\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),  # Learning rate for gradient boosting\n",
        "        'depth': trial.suggest_int('depth', 3, 10),  # Depth of the trees\n",
        "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 10.0),  # L2 regularization coefficient\n",
        "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),  # Temperature for Bayesian bootstrap\n",
        "        'random_seed': RANDOM_SEED,  # Random seed for reproducibility\n",
        "        'verbose': False,  # Whether to print information during training\n",
        "        'task_type':\"GPU\"  # Whether to use GPU for training\n",
        "    }\n",
        "\n",
        "    # Define categorical features for CatBoost\n",
        "    cat_features = ['Gender','family_history_with_overweight','FAVC','FCVC','NCP',\n",
        "                'CAEC','SMOKE','CH2O','SCC','FAF','TUE','CALC','MTRANS']\n",
        "\n",
        "    # Create a pipeline for the CatBoost model with optimized parameters\n",
        "    optuna_model = make_pipeline(\n",
        "                        ExtractFeatures,  # Extract features from data\n",
        "#                         AgeRounder,  # Round age feature\n",
        "#                         HeightRounder,  # Round height feature\n",
        "#                         MEstimateEncoder(cols = raw_cat_cols),  # Encode categorical features\n",
        "                        CatBoostClassifier(**params,cat_features=cat_features)  # CatBoost Classifier\n",
        "                        )\n",
        "\n",
        "    # Perform cross-validation and get mean validation score\n",
        "    val_scores,_,_ = cross_val_model(optuna_model,verbose = False)\n",
        "    return np.array(val_scores).mean()\n",
        "\n",
        "# Create an Optuna study object for maximizing the objective function\n",
        "cat_study = optuna.create_study(direction = 'maximize')"
      ],
      "metadata": {
        "id": "gl00gEgqmKie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters for CatBoost model\n",
        "params = {'learning_rate': 0.13762007048684638, 'depth': 5,\n",
        "          'l2_leaf_reg': 5.285199432056192, 'bagging_temperature': 0.6029582154263095,\n",
        "         'random_seed': RANDOM_SEED,\n",
        "        'verbose': False,\n",
        "        'task_type':\"GPU\",\n",
        "         'iterations':1000}\n",
        "\n",
        "# Create a pipeline for the CatBoost model with specified parameters\n",
        "CB = make_pipeline(\n",
        "                   # Feature preprocessing steps (currently commented out)\n",
        "#                    ExtractFeatures,  # Extract features from data\n",
        "#                    AgeRounder,  # Round age feature\n",
        "#                    HeightRounder,  # Round height feature\n",
        "#                    MEstimateEncoder(cols = raw_cat_cols),  # Encode categorical features\n",
        "#                    CatBoostEncoder(cols = cat_features),  # CatBoost Encoder\n",
        "                   CatBoostClassifier(**params, cat_features=categorical_columns)  # CatBoost Classifier\n",
        "                   )\n",
        "# Train Catboost Model\n",
        "val_scores,val_predictions,test_predictions = cross_val_model(CB)\n",
        "\n",
        "# Store validation predictions in respective categories\n",
        "for k,v in target_mapping.items():\n",
        "    oof_list[f\"cat_{k}\"] = val_predictions[:,v]\n",
        "\n",
        "# Store test predictions in respective categories\n",
        "for k,v in target_mapping.items():\n",
        "    predict_list[f\"cat_{k}\"] = test_predictions[:,v]"
      ],
      "metadata": {
        "id": "FgxPpop0mL4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation\n",
        "\n",
        "It defines weights for different models in an ensemble, where each weight corresponds to a specific machine learning model(Random Forest Classifier, LightGBM Classifier, XGBoost Classifier, and CatBoost Classifier). It then combines the predictions from these models using the specified weights to create an ensemble prediction. The ensemble prediction is evaluated by calculating its accuracy score against the actual labels of the training data. Additionally, it generates a confusion matrix based on the ensemble prediction and the actual labels to visually assess the model's performance. Finally, the confusion matrix is plotted to provide insights into the model's classification performance across different target classes. Overall, this code orchestrates an ensemble approach to leverage the strengths of multiple models for improved predictive performance."
      ],
      "metadata": {
        "id": "WDuFooQUmNK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define weights for different models in the ensemble\n",
        "weights = {\"rfc_\":0,  # Weight for Random Forest Classifier\n",
        "           \"lgbm_\":3,  # Weight for LightGBM Classifier\n",
        "           \"xgb_\":1,  # Weight for XGBoost Classifier\n",
        "           \"cat_\":0}  # Weight for CatBoost Classifier\n",
        "\n",
        "# Make a copy of the oof_list\n",
        "tmp = oof_list.copy()\n",
        "\n",
        "# Combine predictions from different models with specified weights\n",
        "for k,v in target_mapping.items():\n",
        "    tmp[f\"{k}\"] = (weights['rfc_']*tmp[f\"rfc_{k}\"]+  # Combine predictions with Random Forest weight\n",
        "              weights['lgbm_']*tmp[f\"lgbm_{k}\"]+  # Combine predictions with LightGBM weight\n",
        "              weights['xgb_']*tmp[f\"xgb_{k}\"]+  # Combine predictions with XGBoost weight\n",
        "              weights['cat_']*tmp[f\"cat_{k}\"])  # Combine predictions with CatBoost weight\n",
        "\n",
        "# Combine predictions to get ensemble prediction\n",
        "tmp['pred'] = tmp[target_mapping.keys()].idxmax(axis = 1)  # Combine predictions from all models to get ensemble prediction\n",
        "tmp['label'] = train[TARGET]  # Actual labels of the training data\n",
        "\n",
        "# Calculate ensemble accuracy score\n",
        "print(f\"Ensemble Accuracy Scoe: {accuracy_score(train[TARGET],tmp['pred'])}\")\n",
        "\n",
        "# Generate confusion matrix for the ensemble prediction\n",
        "cm = confusion_matrix(y_true = tmp['label'].map(target_mapping),  # Generate confusion matrix based on actual labels\n",
        "                      y_pred = tmp['pred'].map(target_mapping),  # and ensemble prediction\n",
        "                     normalize='true')  # Normalize the confusion matrix\n",
        "\n",
        "cm = cm.round(2)  # Round confusion matrix values\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8,8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix = cm,\n",
        "                              display_labels = target_mapping.keys())\n",
        "disp.plot(xticks_rotation=50)  # Plot confusion matrix with rotated xticks for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hjMk9nOZmPrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission"
      ],
      "metadata": {
        "id": "Jex4wdpPmRSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine predictions from different models with specified weights to generate final predictions\n",
        "for k,v in target_mapping.items():\n",
        "    predict_list[f\"{k}\"] = (weights['rfc_']*predict_list[f\"rfc_{k}\"]+\n",
        "                            weights['lgbm_']*predict_list[f\"lgbm_{k}\"]+\n",
        "                            weights['xgb_']*predict_list[f\"xgb_{k}\"]+\n",
        "                            weights['cat_']*predict_list[f\"cat_{k}\"])\n",
        "\n",
        "# Combine predictions to get final prediction\n",
        "final_pred = predict_list[target_mapping.keys()].idxmax(axis = 1)\n",
        "\n",
        "# Update submission dataframe with final predictions and save it to a CSV file\n",
        "sample_sub[TARGET] = final_pred\n",
        "sample_sub.to_csv(\"submission.csv\",index=False)\n",
        "sample_sub"
      ],
      "metadata": {
        "id": "Ft8EIkNfmTss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}